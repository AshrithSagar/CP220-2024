{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AshrithSagar/CP220-2024/blob/main/1_hw_cp220_Q.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLMB_HQoVKhN"
      },
      "source": [
        "#HW 1\n",
        "Please write your answers in the cells below\n",
        "For help with Latex notation, see this sites:\n",
        "\n",
        "1. https://colab.research.google.com/github/bebi103a/bebi103a.github.io/blob/master/lessons/00/intro_to_latex.ipynb\n",
        "\n",
        "2. https://oeis.org/wiki/List_of_LaTeX_mathematical_symbols\n",
        "\n",
        "For help with Google Colab see:\n",
        "*italicised text*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsRD6CtW2wT3"
      },
      "source": [
        "# Topics\n",
        "\n",
        "Vector Spaces, Basis, Dimension, Subspaces, Affine Spaces, Dot Product, Norm, Cauchy Schwarz Inequality, Angle, Matrices, Transpose, NullSpace, Kernel, Rank\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqE3wpcpljo-"
      },
      "source": [
        "---\n",
        "The set of real numbers, ℝ , along with its operations of addition, multiplication and the special numbers 0 (additive identity) and 1 (multiplicative identity) are the inspiration for defining the alberaic concept of **Field**\n",
        "\n",
        "A field 𝐹 is a set with operations \\+ , · and elements *0*, *1* such that\n",
        "\n",
        "$∀ a,b,c ∈ 𝐹 $\n",
        "\n",
        "Closure under addition and multiplication: $a · b ∈ 𝐹;\\ a + b ∈ 𝐹$\n",
        "\n",
        "Identities: $a + 0 = 0 + a = a;\\ a · 1 = 1 · a = a$\n",
        "\n",
        "Commutativity: $ a + b = b + a;\\ a \\cdotp b = b \\cdotp a$\n",
        "\n",
        "Associativity : $ (a + b) + c = a + ( b + c);\\ (a \\cdotp b) \\cdotp c = a \\cdotp ( b \\cdotp c) $\n",
        "\n",
        "Additive inverse: $∀ a ∈ 𝐹, ∃ -a ∈ 𝐹 \\ such\\ that:\\ a + -a = -a + a = 0$\n",
        "\n",
        "Multiplicative inverse: $∀ a \\neq 0 ∈ 𝐹, ∃\\ a^{-1} ∈ 𝐹\\ such\\ that:\\ a \\cdotp a^{-1} = a^{-1} \\cdotp a = 1$\n",
        "\n",
        "Distributivity of multiplication over addition: $a \\cdotp (b + c) = a \\cdotp b + a \\cdotp c$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDGxQWm-vdJn"
      },
      "source": [
        "---\n",
        "**Examples**:\n",
        "\n",
        "Besides real numbers, other examples of fields are\n",
        "\n",
        "Set of complex numbers $ℂ = \\{x + i y\\}\\ where\\ x, y ∈ ℝ\\ and\\ i=\\sqrt{-1}$ along with its usual definition of multiplication and addition\n",
        "\n",
        "Set of Rationals $ℚ = \\{$ ${p \\over q}$ $\\}$ where $ p, q \\neq 0 ∈ 𝚉$ (integers) along with its usual definition of multiplication and addition\n",
        "\n",
        "A finite set of integers $P = \\{0, 1,\\ ...,\\ p-1\\}$ where $p$ is a prime with addition and multiplication are modulo $p$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWhmfwQEy54I"
      },
      "source": [
        "---\n",
        "**Q1**\n",
        "\n",
        "Show that the set $P= \\{0, 1, 2, 3, 4, 5, 6\\}$ with addition and multiplication modulo 6 is not a Field.\n",
        "\n",
        "Ans:\n",
        "\n",
        "The set $P = \\{0, 1, 2, 3, 4, 5, 6\\}$ with addition and multiplication modulo 6 is not a Field.\n",
        "It is possible to verify that the element $2$ does not have a multiplicative inverse in the set $P$.\n",
        "\n",
        "\\begin{align*}\n",
        "2 * 0 \\mod 6 & = 0 \\\\\n",
        "2 * 1 \\mod 6 & = 2 \\\\\n",
        "2 * 2 \\mod 6 & = 4 \\\\\n",
        "2 * 3 \\mod 6 & = 0 \\\\\n",
        "2 * 4 \\mod 6 & = 2 \\\\\n",
        "2 * 5 \\mod 6 & = 4 \\\\\n",
        "\\end{align*}\n",
        "\n",
        "We can see that none of the results above are equal to $1$, which is the multiplicative identity in the set $P$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oHD6mmNQxjU"
      },
      "source": [
        "---\n",
        "**Q2**\n",
        "\n",
        "Show that $0v = 0$ for all v in V\n",
        "\n",
        "Ans:\n",
        "\n",
        "The question is restated as follows\n",
        "> Show that $0\\mathbf{v} = \\mathbf{0}$ for all $\\mathbf{v}$ in a vector space $V$ over $\\mathbb{R}$.\n",
        "\n",
        "Given $V$ is a vector space and $\\mathbf{v}$ to be a vector in $V$, the left hand side represents the scalar multiplication of $\\mathbf{v}$ with $0$, and the right hand side is the zero vector $\\mathbf{0}$.\n",
        "We start with the property of the reals numbers $\\mathbb{R}$ that $0 + 0 = 0$, following which we get $(0 + 0)\\mathbf{v} = 0\\mathbf{v}$, which is just using the same scalars for multiplication with $\\mathbf{v}$.\n",
        "Now, $(0 + 0)\\mathbf{v} = 0\\mathbf{v} + 0\\mathbf{v}$, by the axiom of distribution over scalar addition.\n",
        "This gives us $0\\mathbf{v} + 0\\mathbf{v} = 0\\mathbf{v}$.\n",
        "Adding the additive inverse of $0\\mathbf{v}$, which is $-0\\mathbf{v}$ [Assume it with a variable $a$ if this is a problem to write as $-0\\mathbf{v}$], to both sides, we get $(0\\mathbf{v} + 0\\mathbf{v}) + (-0\\mathbf{v}) = 0\\mathbf{v} + (-0\\mathbf{v})$.\n",
        "Applying the axiom of associativity of addition to the left hand side, we get $0\\mathbf{v} + (0\\mathbf{v}) + (-0\\mathbf{v}) = (0\\mathbf{v} + (-0\\mathbf{v}))$.\n",
        "Since $(0\\mathbf{v}) + (-0\\mathbf{v}) = 0$, which is from the additive inverse axiom, we finally get $0\\mathbf{v} + 0 = 0$.\n",
        "From this, it is easy to see that the left hand side is $0\\mathbf{v} + 0 = 0\\mathbf{v}$, from the additive identity axiom.\n",
        "Hence, we get $0\\mathbf{v} = 0$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa3U-HfQzVK2"
      },
      "source": [
        "---\n",
        "**Vector Spaces**\n",
        "\n",
        "A set 𝑉 is a vector space over a field  𝐹 , (called scalars) with vector addition \\+ and scalar multiplication if the following hold true:\n",
        "\n",
        "$∀\\ u,v,w \\in 𝑉$ and $a,b \\in 𝐹$\n",
        "\n",
        "Closure under addition: $u + v \\in 𝑉$\n",
        "\n",
        "Closure under scalar multiplication: $a v \\in 𝑉$\n",
        "\n",
        "Addition is commutative: $u + v = v + u$\n",
        "\n",
        "Addition is associative: $u + (v + w) = (u + v) + w$\n",
        "\n",
        "Identity of vector addition: $∃\\ 0 \\in 𝑉$, called zero vector, such that $u + 0 = 0 + u = u$\n",
        "\n",
        "Inverse: $∃ -u \\in 𝑉$ such that $u + -u = -u + u = 0$\n",
        "\n",
        "Compatibility of scalar multiplication with field multiplication: $a(bv) = (ab)v $\n",
        "\n",
        "Identity of scalar multiplication: $1v = v$ where $1$ is multiplicative identity of 𝐹\n",
        "\n",
        "Distributivity of scalar multiplication over vector addition: $a(u +v) = au + av$\n",
        "\n",
        "Distributivity of scalar multiplication over field addition: $(a+b)v = av + bv$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoqKGApt2TW0"
      },
      "source": [
        "---\n",
        "**Examples**\n",
        "\n",
        "1. Set of reals ℝ over the scalar field ℝ.\n",
        "\n",
        "2. The 2D Euclidean space  $ℝ^2$ over ℝ\n",
        "\n",
        "3. The 3D Eucliean space   $ℝ^3$ over ℝ\n",
        "\n",
        "4. N-dimensional space   $ℝ^n$ over ℝ\n",
        "\n",
        "5. The set of complex numbers, ℂ over ℝ\n",
        "\n",
        "6. The set of complex numbers, ℂ over ℂ\n",
        "\n",
        "7. The set of real numbers ℝ over set of rationals ℚ\n",
        "\n",
        "8. The physical entities: displacement, velocity, acceleration, force, electric field, magnetic field etc. can be modeled as 3D vectors over ℝ\n",
        "\n",
        "9. The set of continuous functions $f: ℝ → ℝ $ is a vector space over ℝ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXimMqXX29mG"
      },
      "source": [
        "---\n",
        "**Q3**\n",
        "\n",
        "Which of the following are vector spaces over 𝑅? Explain\n",
        "\n",
        "a. All sequences like (1, 0, 1, ... which have infinitely many zeros.\n",
        "\n",
        "Ans:\n",
        "\n",
        "No, this is not a vector space over $\\mathbb{R}$.\n",
        "Consider two elements in the set of sequences, namely $(1, 0, 1, ...)$ and $(0, 1, 0, 1, ...)$, the sum of both which is $(1, 1, 1, ...)$ is not in the set, hence this won't be a vector space.\n",
        "\n",
        "\n",
        "b. All sequences $(x_1, x_2, ...)$ with $x_j=0 ∀ j > k$ for some $k$\n",
        "\n",
        "Ans:\n",
        "\n",
        "Yes, this is a vector space over $\\mathbb{R}$.\n",
        "We can show that $\\mathbb{R}^k$ is a subspace of this set, and the it then follows from the fact that $\\mathbb{R}^k$ is a vector space over $\\mathbb{R}$.\n",
        "\n",
        "Elaboratly, we can show that all the conditions required for a vector space are satisfied by this set.\n",
        "\n",
        "$$\n",
        "V = \\{ (x_1, x_2, \\dots) | x_j = 0 \\ \\forall j > k, \\ \\text{for some} \\ k \\}\n",
        "$$\n",
        "\n",
        "Note that $x_j \\in \\mathbb{R} \\ \\forall j$.\n",
        "\n",
        "Let $\\mathbf{u} = (u_1, u_2, \\dots)$, $\\mathbf{v} = (v_1, v_2, \\dots)$ and $\\mathbf{w} = (w_1, w_2, \\dots)$ be elements in $V$.\n",
        "Let $a, b \\in \\mathbb{R}$ be two scalars.\n",
        "Now, we will verify all the conditions required for a vector space:\n",
        "\n",
        "- Closure under addition: $\\mathbf{u} + \\mathbf{v} \\in V$.\n",
        "\n",
        "    We can see that $\\mathbf{u} + \\mathbf{v} = (u_1 + v_1, u_2 + v_2, \\dots)$ is also in $V$.\n",
        "\n",
        "- Closure under scalar multiplication: $a\\mathbf{u} \\in V$.\n",
        "\n",
        "    We can see that $a\\mathbf{u} = (au_1, au_2, \\dots)$ is also in $V$.\n",
        "\n",
        "- Addition is commutative: $\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}$\n",
        "\n",
        "    $\\mathbf{u} + \\mathbf{v} = (u_1 + v_1, u_2 + v_2, \\dots)$\n",
        "\n",
        "    Since $u_i + v_i = v_i + u_i$, as addition is commutative in $\\mathbb{R}$.\n",
        "\n",
        "- Addition is associative: $\\mathbf{u} + (\\mathbf{v} + \\mathbf{w}) = (\\mathbf{u} + \\mathbf{v}) + \\mathbf{w}$\n",
        "\n",
        "    $\\mathbf{u} + (\\mathbf{v} + \\mathbf{w}) = (u_1 + (v_1 + w_1), u_2 + (v_2 + w_2), \\dots)$\n",
        "\n",
        "    $(\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = ((u_1 + v_1) + w_1, (u_2 + v_2) + w_2, \\dots)$\n",
        "\n",
        "    Since addition is associative in $\\mathbb{R}$, the above two are equal.\n",
        "\n",
        "- Identity of vector addition:\n",
        "\n",
        "    We can see that the zero vector for $V$ is $(0, 0, \\dots)$,\n",
        "\n",
        "    $\\mathbf{u} + \\mathbb{0} = (u_1 + 0, u_2 + 0, \\dots) = (u_1, u_2, \\dots) = \\mathbf{u}, \\forall \\mathbf{u} \\in V$\n",
        "\n",
        "    Hence, the identity of vector addition exists.\n",
        "\n",
        "- Inverse: $\\exists -\\mathbf{u} \\in V$ such that $\\mathbf{u} + -\\mathbf{u} = -\\mathbf{u} + \\mathbf{u} = \\mathbb{0}$\n",
        "\n",
        "    We can see that the additive inverse of $\\mathbf{u}$ is $(-u_1, -u_2, \\dots)$, which is also in $V$.\n",
        "\n",
        "- Compatibility of scalar multiplication with field multiplication: $a(b\\mathbf{u}) = (ab)\\mathbf{u}$\n",
        "\n",
        "    $a(b\\mathbf{u}) = a(bu_1, bu_2, \\dots) = (abu_1, abu_2, \\dots) = (ab)\\mathbf{u}$\n",
        "\n",
        "- Identity of scalar multiplication: $1\\mathbf{u} = \\mathbf{u}$\n",
        "\n",
        "    $1\\mathbf{u} = 1(u_1, u_2, \\dots) = (u_1, u_2, \\dots) = \\mathbf{u}$\n",
        "\n",
        "- Distributivity of scalar multiplication over vector addition: $a(\\mathbf{u} + \\mathbf{v}) = a\\mathbf{u} + a\\mathbf{v}$\n",
        "\n",
        "    $a(\\mathbf{u} + \\mathbf{v}) = a(u_1 + v_1, u_2 + v_2, \\dots) = (au_1 + av_1, au_2 + av_2, \\dots) = (au_1, au_2, \\dots) + (av_1, av_2, \\dots) = a\\mathbf{u} + a\\mathbf{v}$\n",
        "\n",
        "- Distributivity of scalar multiplication over field addition: $(a + b)\\mathbf{u} = a\\mathbf{u} + b\\mathbf{u}$\n",
        "\n",
        "    $(a + b)\\mathbf{u} = (a + b)(u_1, u_2, \\dots) = ((a + b)u_1, (a + b)u_2, \\dots) = (au_1 + bu_1, au_2 + bu_2, \\dots) = (au_1, au_2, \\dots) + (bu_1, bu_2, \\dots) = a\\mathbf{u} + b\\mathbf{u}$\n",
        "\n",
        "\n",
        "c. All geometric progressions $(x_1, kx_1, k^2x_1, ...) for all k, x_1 \\in ℝ$\n",
        "\n",
        "Ans:\n",
        "\n",
        "No, this is not a vector space over $\\mathbb{R}$.\n",
        "Sum of two geometric progressions element by element, won't result in a geometric progression.\n",
        "As a consequence, the closure property of vector spaces won't satisfied.\n",
        "\n",
        "\n",
        "d. All solutions $(x , y , z) of ax + by + cz = 0$\n",
        "\n",
        "Ans:\n",
        "\n",
        "We can verify all the conditions required for a vector space, as below:\n",
        "\n",
        "- Closure under addition:\n",
        "\n",
        "    If $(x, y, z)$ and $(x', y', z')$ are solutions to $ax + by + cz = 0$, then $a(x + x') + b(y + y') + c(z + z') = 0$, which is also a solution.\n",
        "\n",
        "- Closure under scalar multiplication:\n",
        "\n",
        "    If $(x, y, z)$ is a solution to $ax + by + cz = 0$, then it follows that $\\alpha (ax + by + cz) = \\alpha \\times 0 \\implies a(\\alpha x) + b(\\alpha y) + c(\\alpha z) = 0$, which is also a solution, for $\\alpha \\in \\mathbb{R}$.\n",
        "\n",
        "- Addition is commutative:\n",
        "\n",
        "    This follows since addition is commutative in $\\mathbb{R}$.\n",
        "\n",
        "    If $(x, y, z)$ and $(x', y', z')$ are solutions to $ax + by + cz = 0$, then $a(x + x') + b(y + y') + c(z + z') = a(x' + x) + b(y' + y') + c(z' + z ) = 0$.\n",
        "\n",
        "- Addition is associative:\n",
        "\n",
        "    This again follows from assosiative property of addition over $\\mathbb{R}$.\n",
        "\n",
        "- Identity of vector addition:\n",
        "\n",
        "    We can see that $(0, 0, 0)$ is a trivial solution to $ax + by + cz = 0$, i.e., the zero vector exists in $V$.\n",
        "\n",
        "    If $(x, y, z)$ is a solution to $ax + by + cz = 0$, then $a(x + 0) + b(y + 0) + c(z + 0) = 0$, which is also a solution.\n",
        "\n",
        "- Inverse:\n",
        "\n",
        "    If $(x, y, z)$ is a solution to $ax + by + cz = 0$, then $a(-x) + b(-y) + c(-z) = 0$, which is also a solution.\n",
        "\n",
        "    And $(x, y, z) + (-x, -y, -z) = (0, 0, 0)$, giving the identity vector.\n",
        "\n",
        "- Compatibility of scalar multiplication with field multiplication:\n",
        "\n",
        "    If $(x, y, z)$ is a solution to $ax + by + cz = 0$, then it follows that $\\alpha (\\beta (ax + by + cz)) = \\alpha (\\beta \\times 0) = \\alpha \\times (0) = 0$, for $\\alpha, \\beta \\in \\mathbb{R}$.\n",
        "\n",
        "    This is the same as doing $(\\alpha \\times \\beta) \\times (ax + by + cz)) = (\\alpha \\times \\beta) \\times (0) = 0$.\n",
        "    Hence, the condition is satisfied.\n",
        "\n",
        "- Identity of scalar multiplication:\n",
        "\n",
        "    If $(x, y, z)$ is a solution to $ax + by + cz = 0$, then it follows that $1(ax + by + cz) = 1 \\times 0 = 0$, which is also a solution.\n",
        "\n",
        "- Distributivity of scalar multiplication over vector addition:\n",
        "\n",
        "- Distributivity of scalar multiplication over field addition:\n",
        "\n",
        "\n",
        "e. All solutions $(x , y , z)$ of $ax + by + cz = 5$\n",
        "\n",
        "Ans:\n",
        "\n",
        "No, this is not a vector space over $\\mathbb{R}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11zjY6VoNhYh"
      },
      "source": [
        "---\n",
        "**Linear Subspace or Vector Subspace**\n",
        "\n",
        "If 𝑉 is a vector space over 𝐹 then 𝑊 ⊂ 𝑉 is a linear subspace if it is closed under scalar multiplication and vector addition.\n",
        "\n",
        "Examples:\n",
        "\n",
        "1) The plane $\\{(x,y,z) | ax + by + cz = 0 \\}$ is a subspace of  $ℝ^3$\n",
        "\n",
        "2) The set of continous function from real to real is a subspace of the vector space of all functions from real to real\n",
        "\n",
        "3) Consider the set $P(a,b,c) = \\{ (\\lambda a, \\lambda b, \\lambda c)\\}$ $∀ \\lambda \\in ℝ $ and some fixed $a, b, c \\in ℝ$. $P(a,b,c) $ is a subspace of $ℝ^3$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abzM6Eu1SW2a"
      },
      "source": [
        "---\n",
        "**Q4**\n",
        "\n",
        "Show that the intersection of subspaces is a subspace\n",
        "\n",
        "Ans:\n",
        "\n",
        "Consider two subspaces $S_1$ and $S_2$ of a vector space $V$.\n",
        "\n",
        "Since $\\mathbf{0} \\in S_1$ and $\\mathbf{0} \\in S_2 \\implies \\mathbf{0} \\in S_1 \\cap S_2$.\n",
        "Therefore $S_1 \\cap S_2$ is a *non-empty set*.\n",
        "\n",
        "Let $\\mathbf{s_1}, \\mathbf{s_2} \\in S_1 \\cap S_2$.\n",
        "This implies $\\mathbf{s_1}, \\mathbf{s_2} \\in S_1$ and $\\mathbf{s_1}, \\mathbf{s_2} \\in S_2$, since $S_1 \\cap S_2 \\subseteq S_1$ and $S_1 \\cap S_2 \\subseteq S_2$, respectively.\n",
        "\n",
        "From $\\mathbf{s_1}, \\mathbf{s_2} \\in S_1$ and that $S_1$ is a subspace of $V$, we can see that $\\mathbf{s_1} + \\mathbf{s_2} \\in S_1$.\n",
        "Similarly, $\\mathbf{s_1} + \\mathbf{s_2} \\in S_2$.\n",
        "Therefore, $\\mathbf{s_1} + \\mathbf{s_2} \\in S_1 \\cap S2$.\n",
        "This shows the *closure under addition*.\n",
        "\n",
        "Now, let $\\mathbf{s} \\in S_1 \\cap S_2$ and $\\alpha \\in F$, and similar to above, we have $\\mathbf{s} \\in S_1$ and $\\mathbf{s} \\in S_2$.\n",
        "Now, $\\alpha \\mathbf{s} \\in S_1$ and $\\alpha \\mathbf{s} \\in S_2$, since $S_1$ and $S_2$ are subspaces of $V$.\n",
        "We can see that $\\alpha \\mathbf{s} \\in S_1 \\cap S_2$, hence $S_1 \\cap S_2$ is *closed under scalar multiplication*.\n",
        "\n",
        "It follows that $S_1 \\cap S_2$ is a subspace of $V$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0Dm9K1jTfA-"
      },
      "source": [
        "---\n",
        "**Q5**\n",
        "\n",
        "Is a union of subspaces a vector space?\n",
        "\n",
        "Ans:\n",
        "\n",
        "Not necessarily.\n",
        "We will show that the union of two subspaces will be a vector space only when one of them is a subset of the other.\n",
        "\n",
        "Consider two subspaces $S_1$ and $S_2$ of a vector space $V$.\n",
        "\n",
        "If $S_1 \\subseteq S_2$, then $S_1 \\cup S_2 = S_1$, which is a vector space.\n",
        "Similarly, if $S_2 \\subseteq S_1$, then $S_1 \\cup S_2 = S_2$, which is a vector space.\n",
        "\n",
        "Next, we have to consider the case where there exists atleast one vector not in both $S_1$ and $S_2$, i.e., suppose that $S_1 \\nsubseteq S_2$ and $S_2 \\nsubseteq S_1$.\n",
        "Let $\\mathbf{v_1}$ and $\\mathbf{v_2}$ be such vectors such that $\\mathbf{v_1} \\in S_1$ and $\\mathbf{v_1} \\notin S_2$, and $\\mathbf{v_2} \\in S_2$ and $\\mathbf{v_2} \\notin S_1$.\n",
        "Then we will show that $\\mathbf{v_1} + \\mathbf{v_2} \\notin S_1 \\cup S_2$, thereby violating the closure property of vector spaces.\n",
        "We will show this by proof of contradiction.\n",
        "\n",
        "On the contrary, suppose that $S_1 \\cup S_2$ is a vector space $\\implies \\mathbf{v_1} + \\mathbf{v_2} \\in S_1 \\cup S_2$.\n",
        "This can imply one of the following cases, where $\\mathbf{v_1} + \\mathbf{v_2} \\in S_1$ or $\\mathbf{v_1} + \\mathbf{v_2} \\in S_2$, or both.\n",
        "\n",
        "If $\\mathbf{v_1} + \\mathbf{v_2} \\in S_1$, we have $\\mathbf{v_3} = \\mathbf{v_1} + \\mathbf{v_2} \\implies \\mathbf{v_2} = \\mathbf{v_3} - \\mathbf{v_1}$, i.e. $\\mathbf{v_2} = \\mathbf{v_3} + (-1)\\mathbf{v_1} \\implies \\mathbf{v_2} \\in S_1$, which is a contradiction.\n",
        "Similarly, if $\\mathbf{v_1} + \\mathbf{v_2} \\in S_2$, we get $\\mathbf{v_1} \\in S_2$, again a contradiction.\n",
        "Hence, $S_1 \\cup S_2$ is not a vector space, in this case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnPvJau0UCWd"
      },
      "source": [
        "---\n",
        "**Linear Combination**\n",
        "\n",
        "Let $G = {g_1, g_2, ...., g_k} ⊂ 𝑉$\n",
        "then a linear combination of elements of $G$ is:\n",
        "\n",
        "$a_1 g_1 + a_2 g_2 + ... + a_k g_k$\n",
        "\n",
        "where $a_1, a_2, ..., a_k$ are scalars from 𝐹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcWQ5pXPUmgF"
      },
      "source": [
        "---\n",
        "**Linear Independance**\n",
        "\n",
        "$u$ is linearly independant of $\\{v_1, v_2, .... v_k, ... \\}$ if it cannot be expressed as a linear combination of them.\n",
        "\n",
        "Similarly, the set of vectors $\\{v_1, v_2, .... v_k, .... \\}$  is linearly independant if no element of the set can be expressed as a linear combination of the rest.\n",
        "\n",
        "Equivalenty,\n",
        "\n",
        "the set of vectors $\\{v_1, v_2, .... v_k , .... \\}$  is linearly independant if the 0 vector can be obtained only  as a linear combination with zero scalar coefficients.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic2dc7evWWlz"
      },
      "source": [
        "---\n",
        "**Linear Span**\n",
        "\n",
        "Let $G=\\{g_1, g_2, ....g_k \\}$ be a subset of vectors. Then\n",
        "\n",
        "Span(G) = $\\{ a_1 g_1 + a_2 g_2 + ... + a_k g_k\\}$ for all scalars $a_1, a_2, ,... a_k$ from the field.\n",
        "\n",
        "Span(G) is generated by linear combinations of G\n",
        "\n",
        "Span(G) ⊂ 𝑉 and is also a vector subspace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiUUEnDOXGv0"
      },
      "source": [
        "---\n",
        "**Basis** and **Dimension**\n",
        "\n",
        "A basis $B ⊂ 𝑉$ is a linearly independant set that spans 𝑉.\n",
        "\n",
        "ie. Span($B$) = 𝑉\n",
        "\n",
        "The cardinality of the $B$ (=$|B|$) is the dimension of 𝑉.\n",
        "\n",
        "There can be many possible Basis sets - but all will have same cardinality\n",
        "\n",
        "Examples:\n",
        "\n",
        "1) In 1D, ℝ is a vector space of ℝ and has dimension of 1. The basis is any non-zero number.\n",
        "\n",
        "2) In 3D space, the unit vectors along orthogonal axes defines one basis\n",
        "\n",
        "3) For the set of square integrable functions over a unit interval, the harmonic set of sinusoid and cosinusoids form the basis.\n",
        "\n",
        "$f: [0,1] → ℝ $ is square integrable, i.e $\\int_{0}^{1} |f|^2 dx < ∞$\n",
        "\n",
        "then\n",
        "\n",
        " $f(t) = a_0 + \\sum_{i=1}^{∞} a_i sin(2πit) + b_i cos(2πit)$\n",
        "\n",
        " by Fourier series expansion and\n",
        "\n",
        "  $\\{1, sin(2πit), cos(2πit)\\}$ for $i=1,2,...$ is the infinite basis set.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFcF_vWH_g63"
      },
      "source": [
        "**Q6**\n",
        "\n",
        "What are the dimensions of the following vector spaces?\n",
        "\n",
        "a) A set of (position, velocity) of a train on a track\n",
        "\n",
        "Ans: 2-dimensional\n",
        "\n",
        "b) The set of position, orientation (attitude), velocities and angular velocities of a drone\n",
        "\n",
        "Ans: 12-dimensional\n",
        "\n",
        "Each of the attributes are 3-dimensional, with 4 of them here.\n",
        "\n",
        "c) The space of ℝ over the field 𝑄 (rationals)\n",
        "\n",
        "Ans: Uncountably infinite dimensions\n",
        "\n",
        "The dimension of this space is referred to as the *cardinality of the continuum*, denoted by $\\mathfrak{c}$.\n",
        "\n",
        "d) Subspace $P(a,b,c) = \\{ (\\lambda a, \\lambda b, \\lambda c)\\}$ $∀ \\lambda \\in ℝ $ and some fixed $a, b, c \\in ℝ$.\n",
        "\n",
        "Ans: 1-dimensional\n",
        "\n",
        "This represents a line passing through origin in $\\mathbb{R}^3$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG3N9yEBA_hk"
      },
      "source": [
        "---\n",
        "**Coordinate Representation**\n",
        "\n",
        "Since any vector can be expressed as a linear combination of the elements of the basis set, the linear coefficients provide a representation of the vector in that basis.\n",
        "\n",
        "Considering a finite dimensional vector space with dimension $n$, Let its basis be $B={u_1, u_2, ...., u_n}$\n",
        "\n",
        "Then any vector $v = \\sum_{1}^{n} a_i u_i$\n",
        "\n",
        "here ${a_i}$ are scalars. Hence $(a_1, a_2, ..., a_n)$ is the coordinate representation of $v$ in basis $B$\n",
        "\n",
        "Then $v = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ . \\\\ . \\\\ a_n \\end{bmatrix} $\n",
        "\n",
        "is a way to represent the vector in a matrix form. The matrix is a rectangular array of elements and has size $n x 1$, i.e n rows and 1 column.\n",
        "\n",
        "Examples:\n",
        "\n",
        "a) In 3D Euclidean space, with usual orthogonal axis, any vector is represented as a 3-tuple $(a, b, c)$\n",
        "\n",
        "b) in 2D projective geometry, any 2D point is the subspace $P(a,b,c) = \\{ (\\lambda a, \\lambda b, \\lambda c)\\}$ $∀ \\lambda \\in ℝ $ and some fixed $a, b, c \\in ℝ$ and can be represented as $(a/c,b/c,1)$. This is called the homogenous representation of a point.\n",
        "\n",
        "Similarly a 3D point is mapped to a linear subspace of $ℝ^4$ and represented as $( {a \\over d}, {b \\over d}, {c \\over d}, 1)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIeq8yF_C3hp"
      },
      "source": [
        "**Scalar Product, Dot Product and Inner Product**\n",
        "\n",
        "An scalar product of two vectors is a function relating the two vectors to a scalar (from its field).\n",
        "\n",
        "$· : 𝑉 x 𝑉 → 𝐹\\  i.e.\\   \\forall\\ u, v \\in 𝑉,\\  u · v \\in 𝐹$\n",
        "\n",
        "When the scalar field 𝐹 is real, the scalar product is commutative or symmetric i.e.\n",
        "\n",
        "$u \\cdotp v = v \\cdotp u$\n",
        "\n",
        "When the scalar field 𝐹 is complex, the scalar product has conjugate symmetry. i.e\n",
        "\n",
        "$u \\cdotp v = \\overline{v \\cdotp u}$\n",
        "\n",
        "The scalar product is linear in the second argument, i.e.\n",
        "\n",
        "$u \\cdotp (av + bw) = a\\  u \\cdotp v + b\\ u \\cdotp w$\n",
        "\n",
        "Hence it is conjugate linear in first argument (for real scalar field, it is linear in first argument)\n",
        "\n",
        "By defining the scalar product on all pairs of basis vectors, we can then calculate the scale product on any pair of vectors by the linear expansion and  linearity property of the scalar product.\n",
        "\n",
        "Examples:\n",
        "\n",
        "a) In 3D Euclidean space, let $e_1, e_2, e_3$ be the basis vectors. Let $a = a_1 e_1 + a_2 e_2 + a_3 e_3$ and $b = b_1 e_1 + b_2 e_2 + b_3 e_3$ be the linear expansion of two vectors in terms of the basis. Let the scalar product on the bases vectors be defined as $ e_i · e_j = 0$ if $i \\neq j$ and $e_i · e_i = 1$ for $i = 1,2,3$. Such a basis is called the unit orthogonal basis. Then by linearity property of scalar product:\n",
        "\n",
        "$a · b = a_1b_1 + a_2b_2 + a_3b_3$ and is also called the **dot product**\n",
        "\n",
        "---\n",
        "**inner product**\n",
        "\n",
        "When the scalar product also has this additional property of positive definiteness, it is called an inner product\n",
        "\n",
        "$a · a \\geq 0$ and is equal to 0 only if $a = 0$\n",
        "\n",
        "It is also written in bra-ket notation:\n",
        "\n",
        "<a, b>\n",
        "\n",
        "The vector space with an inner product is also called an inner product space.\n",
        "\n",
        "Exampe:\n",
        "a) The familiar 2D and 3D Euclidean space\n",
        "\n",
        "b) Let ℱ be the space of square integrable real functions in the interval $[0,1]$. Let $f, g \\in ℱ\\ over\\ ℝ$ be two arbitrary functions.\n",
        "\n",
        "Then we can define an inner product as:\n",
        "\n",
        "$<f,g> = \\int_{0}^{1} fgdx$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktcXMKYHoL9O"
      },
      "source": [
        "---\n",
        "**Cauchy-Schwarz Inequality**\n",
        "\n",
        "$\\forall x, y \\in 𝑉$, an vector space over reals or complex nimbers, with inner product, then\n",
        "\n",
        "$|<u,v>|^2\\  \\leq\\  <u,u> \\cdotp <v,v>$\n",
        "\n",
        "with equality if and only if $u,v$ are linearly dependent.\n",
        "\n",
        "\n",
        "See https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality for some  proofs (also in the reference text by Boyd on Applied Linear Algebra)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDa9s7g-g-Wd"
      },
      "source": [
        "---\n",
        "**norm**\n",
        "\n",
        "Once can define a length or norm on any vector space over real or complex numbers as as $\\lVert · \\rVert: 𝑉 → ℝ $  with the following properties $\\forall x, y \\in 𝑉$:\n",
        "\n",
        "a) Non negativity:  $\\lVert x \\rVert \\geq 0$\n",
        "\n",
        "b) Positive definiteness: $\\lVert x \\rVert= 0$ if and only if $x = 0$\n",
        "\n",
        "c) Absolution Homogenity: $\\forall a \\in ℝ\\ or\\ ℂ$, $\\lVert ax \\rVert = |a|\\lvert x \\rVert$\n",
        "\n",
        "d) Triangle inequality: $\\lVert x + y \\rVert \\leq \\lVert x \\rVert + \\lVert y \\rVert$\n",
        "\n",
        "\n",
        "The inner product *induces* a natural  definition of the *length* or *norm* of any vector as:\n",
        "\n",
        "$\\lVert v \\rVert = \\sqrt{<v,v>}$\n",
        "\n",
        "In the case of an Euclidean space with the dot product,\n",
        "\n",
        "Let $v = (v_1, v_2, ..... v_n)$ is the coordinate representation in a unit orthogonal basis. Then\n",
        "\n",
        "$\\lVert v \\rVert = \\sqrt{v_1^2 + v_2^2 + ...+ v_n^2}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFfcX1JHqM2F"
      },
      "source": [
        "---\n",
        "**Other Norms**\n",
        "\n",
        "Other norms commonlly used are\n",
        "\n",
        "$\\lVert x \\rVert_p = (|x_1|^p + |x_2|^p + ....+ |x_n|^p)^\\frac{1}{p} $ called the $L_p$ norm\n",
        "\n",
        "p=2 is the familiar Euclidean distance ($L_2$)\n",
        "\n",
        "$L_∞$ emerges as $\\lVert x \\rVert_∞ = max\\{|x_1|, |x_2|, ... |x_n|\\}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4dhrH236qSY"
      },
      "source": [
        "---\n",
        "**Weighted $L_2$ Norm**\n",
        "\n",
        "Let $w_1, w_2, ... w_n$ be non-negative scalars. Then\n",
        "\n",
        "$\\lVert x \\rVert_w = \\sum_i^n w_i x_i^2$\n",
        "\n",
        "This can be used to assign importance to different components.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9ShCdyj1p4o"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Q7**\n",
        "\n",
        "Show that the $L_1$ and $L_∞$ satisfy the norm properties\n",
        "\n",
        "Ans:\n",
        "\n",
        "The $L_1$ norm of a vector $\\mathbf{x} = (x_1, x_2, \\dots, x_n)$, denoted by $\\lVert \\mathbf{x} \\rVert _{1}$, is defined as a function\n",
        "$\\lVert \\cdot \\rVert _{1}: \\mathbb{R}^n \\rightarrow \\mathbb{R}$\n",
        "$$ \\lVert \\mathbf{x} \\rVert _{1} = \\sum_{i=1}^{n} |x_i| $$\n",
        "and satisfies the norm properties:\n",
        "\n",
        "- Non negativity: $\\lVert \\mathbf{x} \\rVert _{1} \\geq 0$\n",
        "\n",
        "    This follows from the fact that the absolute value is always non-negative, $|a| \\geq 0 \\ \\forall a$.\n",
        "\n",
        "    $$\\implies \\lVert \\mathbf{x} \\rVert _{1} = \\sum_{i} |x_i| \\geq 0$$\n",
        "\n",
        "- Positive definiteness: $\\lVert \\mathbf{x} \\rVert _{1} = 0 \\iff \\mathbf{x} = \\mathbf{0}$\n",
        "\n",
        "    - Suppose that $\\lVert \\mathbf{x} \\rVert _{1} = 0$, we have to show that $\\mathbf{x} = \\mathbf{0}$.\n",
        "\n",
        "        $$\\lVert \\mathbf{x} \\rVert _{1} = 0 \\implies \\sum_{i=1}^{n} |x_i| = 0 \\implies |x_i| = 0 \\ \\forall i \\implies x_i = 0 \\ \\forall i \\implies \\mathbf{x} = \\mathbf{0} $$\n",
        "\n",
        "        This also uses the fact that the absolute value is always non-negative, i.e., $|x_i| \\geq 0 \\ \\forall i$.\n",
        "\n",
        "    - Suppose that $\\mathbf{x} = \\mathbf{0}$, we have to show that $\\lVert \\mathbf{x} \\rVert _{1} = 0$.\n",
        "\n",
        "        $$ \\mathbf{x} = \\mathbf{0} \\implies x_i = 0 \\ \\forall i \\implies \\lVert \\mathbf{x} \\rVert _{1} = \\sum_{i=1}^{n} |x_i| = 0 $$\n",
        "\n",
        "- Absolution Homogenity: $\\lVert a \\mathbf{x} \\rVert _{1} = |a| \\ \\lVert \\mathbf{x} \\rVert _{1}, \\forall a \\in \\mathbb{R}$\n",
        "\n",
        "    This follows from the fact that $|a x| = |a| \\ |x| \\ \\forall a, x \\in \\mathbb{R}$.\n",
        "\n",
        "    $$ \\implies \\lVert a \\mathbf{x} \\rVert _{1} = \\sum_{i=1}^{n} |ax_i| = \\sum_{i=1}^{n} |a| \\ |x_i| = |a| \\ \\sum_{i=1}^{n} |x_i| = |a| \\ \\lVert \\mathbf{x} \\rVert _{1} $$\n",
        "\n",
        "- Triangle inequality: $\\lVert \\mathbf{x} + \\mathbf{y} \\rVert _{1} \\leq \\lVert \\mathbf{x} \\rVert _{1} + \\lVert \\mathbf{y} \\rVert _{1}$\n",
        "\n",
        "    This follows from the fact that $|x + y| \\leq |x| + |y| \\ \\forall x, y \\in \\mathbb{R}$.\n",
        "\n",
        "    $$ \\lVert \\mathbf{x} + \\mathbf{y} \\rVert _{1} = \\sum_{i=1}^{n} |x_i + y_i| \\leq \\sum_{i=1}^{n} |x_i| + |y_i| = \\sum_{i=1}^{n} |x_i| + \\sum_{i=1}^{n} |y_i| = \\lVert \\mathbf{x} \\rVert _{1} + \\lVert \\mathbf{y} \\rVert _{1} $$\n",
        "\n",
        "---\n",
        "\n",
        "Similarly, for the $L_\\infty$ norm, we have as below.\n",
        "\n",
        "The $L_\\infty$ norm of a vector $\\mathbf{x} = (x_1, x_2, \\dots, x_n)$, denoted by $\\lVert \\mathbf{x} \\rVert _{\\infty}$, is defined as a function\n",
        "$\\lVert \\cdot \\rVert _{\\infty}: \\mathbb{R}^n \\rightarrow \\mathbb{R}$\n",
        "$$ \\lVert \\mathbf{x} \\rVert _{\\infty} = \\max_{i} |x_i| $$\n",
        "and satisfies the norm properties:\n",
        "\n",
        "- Non negativity: $\\lVert \\mathbf{x} \\rVert _{\\infty} \\geq 0$\n",
        "\n",
        "    This again follows from the fact that the absolute value is always non-negative, $|a| \\geq 0 \\ \\forall a$.\n",
        "\n",
        "    $$\\implies \\lVert \\mathbf{x} \\rVert _{\\infty} = \\max_{i} |x_i| \\geq 0$$\n",
        "\n",
        "- Positive definiteness: $\\lVert \\mathbf{x} \\rVert _{\\infty} = 0 \\iff \\mathbf{x} = \\mathbf{0}$\n",
        "\n",
        "    - Suppose that $\\lVert \\mathbf{x} \\rVert _{\\infty} = 0$, we have to show that $\\mathbf{x} = \\mathbf{0}$.\n",
        "\n",
        "        $$\\lVert \\mathbf{x} \\rVert _{\\infty} = 0 \\implies \\max_{i} |x_i| = 0 \\implies |x_i| = 0 \\ \\forall i \\implies x_i = 0 \\ \\forall i \\implies \\mathbf{x} = \\mathbf{0} $$\n",
        "\n",
        "        This also uses the fact that the absolute value is always non-negative, i.e., $|x_i| \\geq 0 \\ \\forall i$.\n",
        "\n",
        "    - Suppose that $\\mathbf{x} = \\mathbf{0}$, we have to show that $\\lVert \\mathbf{x} \\rVert _{\\infty} = 0$.\n",
        "\n",
        "        $$ \\mathbf{x} = \\mathbf{0} \\implies x_i = 0 \\ \\forall i \\implies \\lVert \\mathbf{x} \\rVert _{\\infty} = \\max_{i} |x_i| = \\max_{i} |0| = 0 $$\n",
        "\n",
        "- Absolution Homogenity: $\\lVert a \\mathbf{x} \\rVert _{\\infty} = |a| \\ \\lVert \\mathbf{x} \\rVert _{\\infty}, \\forall a \\in \\mathbb{R}$\n",
        "\n",
        "    This again follows from the fact that $|a x| = |a| \\ |x| \\ \\forall a, x \\in \\mathbb{R}$.\n",
        "\n",
        "    $$ \\implies \\lVert a \\mathbf{x} \\rVert _{\\infty} = \\max_{i} |ax_i| = \\max_{i} |a| \\ |x_i| = |a| \\ \\max_{i} |x_i| = |a| \\ \\lVert \\mathbf{x} \\rVert _{\\infty} $$\n",
        "\n",
        "- Triangle inequality: $\\lVert \\mathbf{x} + \\mathbf{y} \\rVert _{\\infty} \\leq \\lVert \\mathbf{x} \\rVert _{\\infty} + \\lVert \\mathbf{y} \\rVert _{\\infty}$\n",
        "\n",
        "    This again follows from the fact that $|x + y| \\leq |x| + |y| \\ \\forall x, y \\in \\mathbb{R}$.\n",
        "\n",
        "    $$ \\lVert \\mathbf{x} + \\mathbf{y} \\rVert _{\\infty} = \\max_{i} |x_i + y_i| \\leq \\max_{i} |x_i| + |y_i| = \\max_{i} |x_i| + \\max_{i} |y_i| = \\lVert \\mathbf{x} \\rVert _{\\infty} + \\lVert \\mathbf{y} \\rVert _{\\infty} $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK2711p-3Yyt"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Angle**\n",
        "\n",
        "Thanks to Cauchy-Schwarz inequality, we can define an angle between two vectors in an inner product space as:\n",
        "\n",
        "$\\cos (\\angle x,y) = \\frac{<x,y>}{\\lVert x \\rVert \\lVert y \\rVert}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4gG4SGU5aTQ"
      },
      "source": [
        "---\n",
        "**Distance**\n",
        "\n",
        "A norm can be used to define the distance between vectors as:\n",
        "\n",
        "$d(x,y) = \\lVert x-y \\rVert$\n",
        "\n",
        "Any distance function shoud have the following properties:\n",
        "\n",
        "$d(x,y) \\geq 0$ and is $0$ if and only if $x = y$ : Positive definites\n",
        "\n",
        "$d(x,y) = d(y,x)$ : Symmetry\n",
        "\n",
        "$d(x,z) \\leq d(x,y) + d(y,z)$ : Triangle inequality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozJ5hDRb6hrO"
      },
      "source": [
        "---\n",
        "**Orthogonality and orthonormal basis**\n",
        "\n",
        "x,y are orthogonal, written as $x \\perp y$, if their scalar product, $<x,y> = 0$\n",
        "\n",
        "This means that $\\cos \\angle {x,y} = 0$ and angle is 90 degrees\n",
        "\n",
        "\n",
        "$e_1, e_2, ...., e_n$ are a orthonormal basis of a vector space of dimension $n$ if they are pairwose orthogonal and each vector as a unit norm.\n",
        "\n",
        "To find the coefficients for any vector as a linear combination of orthonormal basis, is straightforward:\n",
        "\n",
        "Let $x = a_1 e_1 + a_2 e_2 + ... + a_n e_n$\n",
        "\n",
        "then taking the inner product with any basis vector gives the corresponding coefficient\n",
        "\n",
        "$<x, e_i> = a_i$\n",
        "\n",
        "therefore\n",
        "\n",
        "$x = <x,e_1> e_1 + <x, e_2> e_2 + ... + <x,e_n>e_n$\n",
        "\n",
        "Examples:\n",
        "\n",
        "a) The unit vectors along the three perpendicular axis in 3D space are the orthonormal basis of 3D space.\n",
        "\n",
        "b) $\\{1, \\sqrt{2}\\sin 2π kt, \\sqrt{2}\\cos 2πkt \\}_{k=1,2..}$  is an orthonormal basis of square integrable functions in unit interval $[0,1]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ChI2H8hW2Gx"
      },
      "source": [
        "---\n",
        "**Gram-Schmidt Orthogonalization procedure**\n",
        "\n",
        "Givent $n$ vectors $x_1, x_2, ... , x_n$ the procedures constructs an orthonormal set of vectors as follows:\n",
        "\n",
        "$q_1 = x_1$\n",
        "\n",
        "$q_2 = x_2 - <x_2, q_1>q_1$ (subtract out the component along $q_1$ and what remains is orthogonal to $q_1$)\n",
        "\n",
        "$q_3 = x_3 - <x_3, q_2>q_2 - <x_3, q_1>q_1$\n",
        "\n",
        "and so on to find all vectors up to $q_n$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uamywVZ-cTX8"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "**Q8**\n",
        "\n",
        "Let $x = x_1 e_1 + x_2 e_2 + ... x_n e_n$ where $e_1, e_2, ... e_n$ is the orthonormal basis.\n",
        "\n",
        "Find the $L_2$ norm  $\\lVert x \\rVert_2$ in terms of the coefficients $x_1, x_2, ... x_n$\n",
        "\n",
        "Ans:\n",
        "\n",
        "The $L_2$ norm of a vector $\\mathbf{x} = (x_1, x_2, \\dots, x_n)$, denoted by $\\lVert \\mathbf{x} \\rVert _{2}$, is defined as\n",
        "$$ \\lVert \\mathbf{x} \\rVert _{2} = \\sqrt{ \\langle \\mathbf{x}, \\mathbf{x} \\rangle } $$\n",
        "where $ \\langle \\mathbf{x}, \\mathbf{x} \\rangle $ is the inner product of $\\mathbf{x}$ with itself.\n",
        "\n",
        "With the given orthonormal basis, we can write the vector $\\mathbf{x}$ as\n",
        "\n",
        "$$ \\mathbf{x} = x_1 \\mathbf{e}_1 + x_2 \\mathbf{e}_2 + \\dots + x_n \\mathbf{e}_n = \\sum_{i=1}^{n} x_i \\mathbf{e}_i $$\n",
        "\n",
        "The inner product of $\\mathbf{x}$ with itself then becomes\n",
        "\n",
        "$$ \\langle \\mathbf{x}, \\mathbf{x} \\rangle = \\Big\\langle \\sum_{i=1}^{n} x_i \\mathbf{e}_i, \\ \\sum_{j=1}^{n} x_j \\mathbf{e}_j \\Big\\rangle = \\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i x_j \\langle \\mathbf{e}_i, \\mathbf{e}_j \\rangle $$\n",
        "\n",
        "Since the basis vectors are orthonormal, we have\n",
        "\n",
        "$$ \\langle \\mathbf{e}_i, \\mathbf{e}_j \\rangle = \\begin{cases} 1 & \\text{if } i = j \\\\ 0 & \\text{if } i \\neq j \\end{cases} $$\n",
        "\n",
        "$$ \\implies \\langle \\mathbf{x}, \\mathbf{x} \\rangle = \\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i x_j \\langle \\mathbf{e}_i, \\mathbf{e}_j \\rangle = \\sum_{i=1}^{n} x_i^2 $$\n",
        "\n",
        "Therefore, the $L_2$ norm of $\\mathbf{x}$ is\n",
        "$$ \\lVert \\mathbf{x} \\rVert _{2} = \\sqrt{x_1^2 + x_2^2 + \\dots + x_n^2} = \\sqrt{\\sum_{i=1}^{n} x_i^2} $$\n",
        "which is the required expression for the $L_2$ norm in terms of the coefficients $x_1, x_2, \\dots, x_n$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD3HsXL4gkC5"
      },
      "source": [
        "---\n",
        "**Other Scalar Products**\n",
        "\n",
        "Instead of having the all basis vectors have a norm of 1, it is possible to have some of them have a norm of 0 or -1. This leads to other algebras called Clifford Algebras - which are also very interesting and useful to model various physical and mathematical constructs, which will be outside the scope of this course.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Geometric_algebra\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mLGTwD2i6xt"
      },
      "source": [
        "---\n",
        "**Linear Functions**\n",
        "\n",
        "Let $V, W$ be  $n and m$ dimensional vector spaces over a common field (we will assume reals unless otherwise stated)\n",
        "\n",
        "$f:V \\rightarrow W$ is a linear map if $f(a_1 x_1 + a_2 x_2) = a_1f(x_1) + a_2 f(x_2)$ for all scalars $a_1, a_2$ and vectors $x_1, x_2 \\in V$\n",
        "\n",
        "Examples:\n",
        "\n",
        "a) Rotation of a robot maps 3D Eculidean space to itself via a linear map\n",
        "\n",
        "b) Formation of an image on a camera can be modeled as a linear map\n",
        "\n",
        "c) The effect of a current input to a motor leads to a change in motor's angular position and velocity, can be be modeled as a linear map (for sufficiently small current inputs)\n",
        "\n",
        "d) In fact even for any non-linear functions - for small changes in inputs, the effect can be approximated as a linear map\n",
        "\n",
        "e) The formation of a binary hamming code-word if $n+k$ bits given an input of $n$ bits is a linear map (over the field of binary numbers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G3FQ6dCtXQO"
      },
      "source": [
        "---\n",
        "**Q9**\n",
        "\n",
        "Show that a linear transformation must map the $0$ to $0$\n",
        "\n",
        "Ans:\n",
        "\n",
        "We can show this by proof of contradiction.\n",
        "\n",
        "Let $f:V \\rightarrow W$ be a linear map such that $f(\\mathbf{0}) \\neq \\mathbf{0}$.\n",
        "Then, we have $f(\\mathbf{0}) = \\mathbf{v}$ for some $\\mathbf{v} \\in W$.\n",
        "\n",
        "Now, consider the following: $\\mathbf{v} = f(\\mathbf{0}) = f(\\mathbf{0} + \\mathbf{0}) = f(\\mathbf{0}) + f(\\mathbf{0}) = \\mathbf{v} + \\mathbf{v}$.\n",
        "This implies that $\\mathbf{v} = \\mathbf{v} + \\mathbf{v}$ to which adding the additive inverse of $\\mathbf{v}$ on both sides gives $ \\mathbf{v} + (-\\mathbf{v}) = \\mathbf{v} + \\mathbf{v} + (-\\mathbf{v}) \\implies \\mathbf{0} = \\mathbf{v}$, a contradiction.\n",
        "\n",
        "Therefore, $f(\\mathbf{0}) = \\mathbf{0}$ for any linear map $f:V \\rightarrow W$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4HGCJ_Rtvgw"
      },
      "source": [
        "---\n",
        "**Q10**\n",
        "\n",
        "Are these transformations linear?\n",
        "Here vector $v$ has components $v=(v_1, v_2, v_3)$\n",
        "\n",
        "a) $T(v) = (v_2, v_1)$\n",
        "\n",
        "Ans:\n",
        "\n",
        "Let $\\mathbf{v} = (v_1, v_2, v_3)$ and $\\mathbf{w} = (w_1, w_2, w_3)$ be vectors in $V$ and $a, b$ be scalars.\n",
        "\n",
        "Now, we have to check if the transformation $T$ is linear, i.e., if $T(a\\mathbf{v} + b\\mathbf{w}) = aT(\\mathbf{v}) + bT(\\mathbf{w})$.\n",
        "\n",
        "$$ a\\mathbf{v} + b\\mathbf{w} = a(v_1, v_2, v_3) + b(w_1, w_2, w_3) = (av_1 + bw_1, av_2 + bw_2, av_3 + bw_3) $$\n",
        "\n",
        "$$ \\implies T(a\\mathbf{v} + b\\mathbf{w}) = (av_2 + bw_2, av_1 + bw_1) $$\n",
        "\n",
        "$$ aT(\\mathbf{v}) + bT(\\mathbf{w}) = a(v_2, v_1) + b(w_2, w_1) = (av_2 + bw_2, av_1 + bw_1) $$\n",
        "\n",
        "Therefore, $T(a\\mathbf{v} + b\\mathbf{w}) = aT(\\mathbf{v}) + bT(\\mathbf{w})$, hence $T$ is a linear transformation.\n",
        "\n",
        "\n",
        "b) $T(v) = (v_1, v_1)$\n",
        "\n",
        "Ans:\n",
        "\n",
        "$$ T(a\\mathbf{v} + b\\mathbf{w}) = (av_1 + bw_1, av_1 + bw_1) $$\n",
        "\n",
        "$$ aT(\\mathbf{v}) + bT(\\mathbf{w}) = a(v_1, v_1) + b(w_1, w_1) = (av_1 + bw_1, av_1 + bw_1) $$\n",
        "\n",
        "Therefore, $T(a\\mathbf{v} + b\\mathbf{w}) = aT(\\mathbf{v}) + bT(\\mathbf{w})$, hence $T$ is a linear transformation.\n",
        "\n",
        "\n",
        "c) $T(v) = (0,1)$\n",
        "\n",
        "Ans:\n",
        "\n",
        "$$ T(a\\mathbf{v} + b\\mathbf{w}) = (0,1) $$\n",
        "\n",
        "$$ aT(\\mathbf{v}) + bT(\\mathbf{w}) = a(0,1) + b(0,1) = (0, a) + (0, b) = (0, a+b) $$\n",
        "\n",
        "Therefore, $T(a\\mathbf{v} + b\\mathbf{w}) \\neq aT(\\mathbf{v}) + bT(\\mathbf{w})$, hence $T$ is a not linear transformation.\n",
        "\n",
        "\n",
        "d) $T(v) = (\\frac{v_1}{v_3}, \\frac{v_2}{v_3},1)$\n",
        "\n",
        "Ans:\n",
        "\n",
        "$$ T(a\\mathbf{v} + b\\mathbf{w}) = \\Big(\\frac{av_1 + bw_1}{av_3 + bw_3}, \\frac{av_2 + bw_2}{av_3 + bw_3}, 1\\Big) $$\n",
        "\n",
        "$$ aT(\\mathbf{v}) + bT(\\mathbf{w}) = a\\Big(\\frac{v_1}{v_3}, \\frac{v_2}{v_3}, 1\\Big) + b\\Big(\\frac{w_1}{w_3}, \\frac{w_2}{w_3}, 1\\Big) = \\Big(\\frac{av_1}{av_3}, \\frac{av_2}{av_3}, a\\Big) + \\Big(\\frac{bw_1}{bw_3}, \\frac{bw_2}{bw_3}, b\\Big) = \\Big(\\frac{av_1}{av_3} + \\frac{bw_1}{bw_3}, \\frac{av_2}{av_3} + \\frac{bw_2}{bw_3}, a+b\\Big) $$\n",
        "\n",
        "Therefore, $T(a\\mathbf{v} + b\\mathbf{w}) \\neq aT(\\mathbf{v}) + bT(\\mathbf{w})$, hence $T$ is a not linear transformation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGMfDPLSvZxr"
      },
      "source": [
        "---\n",
        "**Q11**\n",
        "\n",
        "Let $T(v) = (v_3, v_1, v_2)$ What is $T^{100}(v)$?\n",
        "\n",
        "Ans:\n",
        "\n",
        "We have $T^2(\\mathbf{v}) = T(T(\\mathbf{v})) = T(v_3, v_1, v_2) = (v_2, v_3, v_1)$, and then $T^3(\\mathbf{v}) = T(T^2(\\mathbf{v})) = T(v_2, v_3, v_1) = (v_1, v_2, v_3) = \\mathbf{v}$.\n",
        "\n",
        "From this, we can see that $T^k(\\mathbf{v}) = T^{k \\mod 3}(\\mathbf{v})$.\n",
        "\n",
        "Therefore, $T^{100}(\\mathbf{v}) = T^{100 \\mod 3}(\\mathbf{v}) = T^1(\\mathbf{v}) = (v_3, v_1, v_2)$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-bQhqwMv2F7"
      },
      "source": [
        "---\n",
        "**Representation of Vectors and Linear Transforms**\n",
        "\n",
        "For finite dimensional vector spaces, vectors and linear transforms can be represented as matrices.\n",
        "\n",
        "Let ${e_1, e_2, ..., e_n}$ be a orthonormal basis of vector space $V$ and let any vector $x = \\sum_{i=1}^n x_i e_i$ then this vector is represented as $x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ ... \\\\ x_n \\end{bmatrix}$\n",
        "\n",
        "which is a column of coefficients. This matrix has n rows and 1 column and hence has size $nx1$\n",
        "\n",
        "The representation of the basis vectors in this basis is:\n",
        "$e_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ ... \\\\ 0 \\end{bmatrix}$ and similarly $e_i$ is a column vector with 1 in the $i^{th}$ row and 0 everywhere.\n",
        "\n",
        "\n",
        "Similarly let $g_1, g_2, ... g_m$ be an orthonormal basis of vector space $W$. Then a linear map $T : V → W$ can be represented by considering how it acts on the basis vectors of $V$.\n",
        "\n",
        "\n",
        "i.e,\n",
        "\n",
        "$T(e_i) = t_{1i} g_1 + t_{2i} g_2 + ... t_{mi} g_m$ and can be represented as a column vector $\\begin{bmatrix} t_{1i} \\\\ t_{2i} \\\\ ... \\\\ t_{mi} \\end{bmatrix}$\n",
        "\n",
        "Now the transformation of any vector $x=\\sum x_i e_i$ can be written as $T(x) = \\sum x_i T(e_i) $\n",
        "\n",
        "We can write $T$ as a 2D matrix with columns being the $T(e_i)$ vectors as $T = \\begin{bmatrix} t_{11} & t_{12} & .. & t_{1n} \\\\ t_{21} & t_{22} & .. & t_{2n} \\\\ ... \\\\ t_{m1} & t_{m2} & .. & t_{mn} \\end{bmatrix}$\n",
        "\n",
        "And the computation of $T(x)$ is done as a matrix vector multiply $T(x) = T x $ where on the right side, T is the $mxn$ matrix and x is the $nx1$ column vector.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNUVrgwGddFR"
      },
      "source": [
        "---\n",
        "**Column Space, Image, Kernel, Null Space, Rank-Nullity**\n",
        "\n",
        "The range or image of the transform (or matrix) A is\n",
        "\n",
        "$Image(A) = \\{y \\in W; y=Ax\\ ∀\\ x \\in V\\}$\n",
        "\n",
        "is a subspace of $W$ as it is span(columns of T) and is also called the **column space**.\n",
        "\n",
        "The set $Kernel(A)=\\{x\\ : Ax =0\\}$ is called the kernel of null space.\n",
        "\n",
        "*Rank-Nullity Theorem*\n",
        "\n",
        "$Dimension(Image(A)) + Dimension(Kernel(A)) = Dimension(V)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GIKM9NAfQqY"
      },
      "source": [
        "---\n",
        "\n",
        "**Transpose of a Matrix**\n",
        "\n",
        "\n",
        "Let $A_{mxn}$ be a matrix $=\\begin{bmatrix} a_{11} & a_{12} & .. & a_{1n} \\\\ a_{21} & a_{22} & .. & a_{2n} \\\\ ... \\\\ a_{m1} & a_{m2} & .. & a_{mn} \\end{bmatrix}$\n",
        "\n",
        "representing the transform $A:V → W$\n",
        "\n",
        "then $A^T$ is a nxm matrix $=\\begin{bmatrix} a_{11} & a_{21} & .. & a_{m1} \\\\ a_{12} & a_{22} & .. & a_{m2} \\\\ ... \\\\ a_{1n} & a_{2n} & .. & a_{mn} \\end{bmatrix}$\n",
        "\n",
        "representing the transform $A^T: W → V$\n",
        "\n",
        "\n",
        "Example:\n",
        "\n",
        "If $x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ .. \\\\ x_n \\end{bmatrix}$ is a column vector\n",
        "\n",
        "$x^T = \\begin{bmatrix} x_1 & x_2 ... & x_n \\end{bmatrix}$ is a row vector\n",
        "\n",
        "\n",
        "Image($A^T$) is the row space of A and is a subspace of V and is the span of rows of $A$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzQTJPD2nJK0"
      },
      "source": [
        "---\n",
        "**Rank of a Matrix**\n",
        "\n",
        "Equivalent Definitions:\n",
        "\n",
        "Largest number of independent columns\n",
        "\n",
        "Dimension of Column space\n",
        "\n",
        "Largest numnber of independent rows\n",
        "\n",
        "Dimension of Row Space\n",
        "\n",
        "For a matrix of $A$ size $m \\times n$, rank($A$) $\\leq\\ min(m,n)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3_CXDCsYHGP"
      },
      "source": [
        "---\n",
        "**Q12**\n",
        "\n",
        "Let $V$ be the space of all polynomials of a single variable upto degree $n$, over the reals.\n",
        "\n",
        "i.e $V = \\{p(x) = a_0 + a_1 x + a_2 x^2 + ... a_n x^n \\}$\n",
        "\n",
        "a) What is the dimension of $V$?\n",
        "\n",
        "Ans: $(n+1)$ dimensional.\n",
        "\n",
        "The basis vectors are $\\{1, x, x^2, ... x^n\\}$\n",
        "\n",
        "\n",
        "b) Consider the differentiation operator $\\frac{d}{dx}:V → V$ such that\n",
        "\n",
        " $ \\frac{d}{dx}p(x) = a_1 + 2a_2 x + 3a_3x^2 + .. na_nx^{n-1}$. Write this operator in a matrix form.\n",
        "\n",
        "Ans:\n",
        "\n",
        "Given\n",
        "\n",
        "$$ \\frac{d}{dx} p(x) = \\sum_{i=1}^{n} i a_i x^{i-1} = \\begin{bmatrix} 0 & 1 & 0 & ... & 0 \\\\ 0 & 0 & 2 & ... & 0 \\\\ ... \\\\ 0 & 0 & 0 & ... & n \\end{bmatrix} \\begin{bmatrix} a_0 \\\\ a_1 \\\\ ... \\\\ a_n \\end{bmatrix} \\begin{bmatrix} 1 & x & x^2 & ... & x^n \\end{bmatrix} $$\n",
        "\n",
        "Therefore, the differentiation operator in matrix form is\n",
        "\n",
        "$$ \\frac{d}{dx} = \\begin{bmatrix} 0 & 1 & 0 & ... & 0 \\\\ 0 & 0 & 2 & ... & 0 \\\\ ... \\\\ 0 & 0 & 0 & ... & n \\end{bmatrix} $$\n",
        "\n",
        "\n",
        "c) What is the rank of the Differentiation matrix?\n",
        "\n",
        "Ans: Rank of the Differentiation matrix is $n$.\n",
        "\n",
        "The rank of the differentiation matrix is the number of independent columns, which is $n$.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
