{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AshrithSagar/CP220-M4RAS-2024/blob/main/3_hw_cp220_Q.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt9kfthZOWZr"
      },
      "source": [
        "**Lab 3**\n",
        "\n",
        "*Topics*\n",
        "\n",
        "Gaussian Elimination <br>\n",
        "Inverse & Non-Singular <br>\n",
        "LU Factorization <br>\n",
        "Least Squares <br>\n",
        "Number representation <br>\n",
        "Orthogonality, Gram-schmidt, Householder <br>\n",
        "Projection Matrices <br>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ5K9GvLqJkV"
      },
      "source": [
        "---\n",
        "**Q1**\n",
        "\n",
        "Let $A=\\begin{bmatrix} a_{:1} & a_{:2} & .. & a_{:n} \\end {bmatrix}$ be a $mxn$ matrix and $B = \\begin{bmatrix} b_{:1}^T \\\\ b_{:2}^T \\\\ .. \\\\ b_{:n}^T \\end{bmatrix}$ be a $nxk$ matrix.\n",
        "\n",
        "Express $AB$ in terms of the products of columns of A and rows of B. What is its dimensions\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-yu1BmylGnD"
      },
      "source": [
        "Ans:\n",
        "\n",
        "We can do so by using the outer product of the columns of $\\mathbf{A}$ and rows of $\\mathbf{B}$.\n",
        "\n",
        "\\begin{equation*}\n",
        "\\mathbf{AB} = \\begin{bmatrix} \\mathbf{a_{:1}} & \\mathbf{a_{:2}} & \\cdots & \\mathbf{a_{:n}} \\end {bmatrix} \\begin{bmatrix} \\mathbf{b_{:1}^T} \\\\ \\mathbf{b_{:2}^T} \\\\ \\vdots \\\\ \\mathbf{b_{:n}^T} \\end{bmatrix}\n",
        "= \\sum_{i=1}^{n} \\mathbf{a_{:i}b_{:i}^T}\n",
        "\\end{equation*}\n",
        "\n",
        "The dimensions of $\\mathbf{AB}$ will be $m \\times k$.\\\n",
        "The dimensions of each of the matrices $\\mathbf{a_{:i}b_{:i}^T} \\ \\forall i$ is also $m \\times k$.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChLXn5kq-ccf"
      },
      "source": [
        "**Review of Gaussian elmination**\n",
        "\n",
        "Given the matrix equation $Ax = b$ with unknown vector $x$ that needs to be determined,\n",
        "\n",
        "Gaussian elimination method proceeds by systematically multiplying the equation on the left side by elementary row matrices - which are lower triangular with all diagonals '1' leading to $A$ becoming upper triangular.\n",
        "\n",
        "$E_1 Ax =  E_1b$\n",
        "\n",
        "$E_2 E_1 Ax = E_2 b$\n",
        "\n",
        "...\n",
        "\n",
        "$E_r E_{r-1} ... E_1 A x = E_r E_{r-1} ... E_1 b$\n",
        "\n",
        "\n",
        "The $E_i$ are chosen from the matrix in the previous step (we will call it $A_{i-1}$ by taking $A_{i-1}[ii]$ as the pivot element, and dividing the ith row by it and scaling and substracting from all rows below it to give 0s in the column below this element. In case this element is itself 0, we can look along the row for the first non-zero element (or the largest non-zero element), swap the two columns and proceed. Note that swapping  columns is equivalent to post-multiplication A by the appropriate permutation matrix.\n",
        "\n",
        "We have seen that the product of lower triangular matrices remains lower triangular and hence $E = E_r E_{r-1} ... E_1 $ is a lower triangular matrix.\n",
        "\n",
        "$EAx = Ux = b$ where $U$ is a upper triangular matrix. We can solve the latter by back substitution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI9bwvdo6nob"
      },
      "source": [
        "---\n",
        "\n",
        "**Inverse of a square matrix**\n",
        "\n",
        "Let $A$ be a square $(nxn)$ matrix. Then if there exists a matrix represented as $A^{-1}$ such that\n",
        "\n",
        "$A^{-1} A = A A^{-1} = I $, the identity matrix, then $A^{-1}$ is the inverse of $A$\n",
        "\n",
        "A is than called a *Non-Singular* matrix. Every full-rank square matrix is non-singular.\n",
        "\n",
        "\n",
        "Note that if $B$ is the left inverse of $A$, i.e. $BA = I$  and $C$ is the right inverse of $A$, i.e $AC = I$ then $B=C=A^{-1}$\n",
        "\n",
        "$BI = B => B(AC) = B => (BA)C = B => IC = C = B$\n",
        "\n",
        "We can see that:\n",
        "\n",
        "$(AB)^{-1} = B^{-1}A^{-1}$\n",
        "\n",
        "Similarly: $(ABC)^{-1} = C^{-1}B^{-1}A^{-1}$ and so on.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt-ZZiiF8F-h"
      },
      "source": [
        "---\n",
        "**Finding inverse via Gaussian Elimination**\n",
        "\n",
        "We need to find $B$ such that $AB = I$, i.e. $A\\begin{bmatrix} b_{:1} & b_{:2} & ... & b_{:n} \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0& ... & 0 \\\\ 0 & 1 & 0 & ... & 0 \\\\ ... \\\\ 0 & 0 & 0 & ... & 1 \\end{bmatrix}$\n",
        "\n",
        "So we need to solve $n simultaneous equations:\n",
        "\n",
        "$A b_{:1} = e_1$\n",
        "\n",
        "$A b_{:2} = e_2$\n",
        "\n",
        "...\n",
        "\n",
        "$Ab_{:n} = e_n$\n",
        "\n",
        "By the Gaussian elimination procedure, we can find a lower triangular matrix $E$ such that $EA = U$ an upper triangular matrix.\n",
        "\n",
        "So we can do $EAB = UB = EI$ and we can solve for each unknown column of $B$ by back substitution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1CO0aMT0moW"
      },
      "source": [
        "---\n",
        "**Inverse of a lower triangular matrix with diagonal 1s**\n",
        "\n",
        "Let $L = \\begin{bmatrix} 1 & 0 & 0 & ... & 0 \\\\ l_{21} & 1 & 0 & ... & 0 \\\\ l_{31} & l_{32} & 1 & ... & 0 \\\\ ... \\\\ l_{n1} & l_{n2} & l_{n3} & ... & 1 \\end{bmatrix}$ be a lower triangular matrix with non-zero diagonal entries\n",
        "\n",
        "\n",
        "Find an $E$ such that $EL = I$\n",
        "\n",
        "If we apply Gaussian elimination process, we get\n",
        "\n",
        "$E_1  = \\begin{bmatrix} 1 & 0 & 0 & ... & 0 \\\\ -l_{21} & 1 & 0 & ... & 0 \\\\ -l_{31} & 0 & 1 &  ... & 0 \\\\ ... \\\\ -l_{n1} & 0 & 0 & ... & 1 \\end{bmatrix}$\n",
        "\n",
        "and so on to give $EL = U=I$ on left side and $EI = E$ on right side. Therefore $E = L^{-1} = \\begin{bmatrix} 1 & 0 & .. & 0 \\\\ -l_{21} & 1 & .. & 0 \\\\ ... \\\\ -l_{n1} & -l_{n2} & .. & 1 \\end{bmatrix} $\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj-FBqWYDyRC"
      },
      "source": [
        "---\n",
        "**LU Factorization**\n",
        "\n",
        "Therefore.  $EAx = Ux = Eb = L^{-1}b$\n",
        "\n",
        "In other words. $EA = L^{-1}A = U => A = LU$\n",
        "\n",
        "Thus we have factorized a square matrix $A$ via Gaussian Elimination into a product of lower triangular and upper triangular matrix.\n",
        "\n",
        "While $L$ has 1s in the diagonal, the upper triangular matrix $U$ might not have. We can pull out the diagonals and write\n",
        "\n",
        "$A = LDU$ where L, U are lower and upper triangular with 1s in the diagonal and D is a diagonal matrix.\n",
        "\n",
        "Note that we can factorize $D=D_1 D_2$ in arbitary ways and hence $A= (LD_1)(D_2U)$ leads to infinitely many LU factorizations $LD_1$ is lower triangular and $D_2U$ is upper triangular.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZqfVHPQqtOJ"
      },
      "source": [
        "---\n",
        "However if $A=LDU$ and $L$ and $U$ are lower and upper triangular respectively with  ones in the diagonal, and if $A$ is invertible, the factorization is unique.\n",
        "\n",
        "If $A$ is invertible, so are $L, D, U$.\n",
        "\n",
        "Let $A = L_1 D_1 U_1 = L_2 D_2 U_2$ be two possible factorizations.\n",
        "\n",
        "Then $D_1U_1U_2^{-1} = L_1^{-1}L_2D_2$.\n",
        "\n",
        "Note that $U_2^{-1}$ is upper triangular and $L_1^{-1}$ is lower triangular. So in the above equation, lower triangular is equal to upper triangular which can only happen if off-diagonal entries are 0 and diagonal entries are same i.e. $D_1 = D_2$\n",
        "\n",
        "This also means that $L_1^{-1}L_2 = I$ which means $L_1 = L_2$. Similarly $U_1 = U_2$\n",
        "\n",
        "Thus if $A$ invertible, there is a unique $LDU$ factorization with $L,U$ lower and upper triangular with diagonals 1.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDUY_geqG9CC"
      },
      "source": [
        "---\n",
        "**Number of multiplications for find LU factorization**\n",
        "\n",
        "$O(n^3)$\n",
        "\n",
        "If we want to use same matrix $A$ repeatedly to solve for different $b$ in $Ax=b$, then we can factorize $A=LU$ and then do\n",
        "\n",
        "$Ly = b$ as forward calculation and $Ux = y$ via back substitition to get $x$.\n",
        "\n",
        "This has $O(n^2)$ multiplications\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5P2lrDuHctG"
      },
      "source": [
        "**Row Echelon Form & Pivots **\n",
        "\n",
        "The structure of $U$ after gaussian elimination tells us which are the linearly independent columns. It has a echelon form (step form) as the boundary between non-zero and zero entries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXZyNtHfiV_4"
      },
      "source": [
        "---\n",
        "**A Brief on Computing Architecture**\n",
        "\n",
        "The computing machine has some arithmetic and logic units (ALU) which are connected to memories. The most significant component of the ALU is the Multiply Accumulate Unit (MAC) which is does a multiply and addition. This is important for matrix-vector multiply (or dot product) :\n",
        "\n",
        "$ s = \\sum_{i=1}^n a_i b_i$ is realized as\n",
        "\n",
        "```python\n",
        "s=0\n",
        "for i in range(n):\n",
        "   s = s + a[i] b[i] #Multiply accumulate\n",
        "```\n",
        "GPU and CPU optimize hardware to speedup this operation - and can be taken advantage of by using vectorized machine instructions. This low level implementation detail is usually hidden in libraries and is exposed as high level API (application programming interfaces).\n",
        "\n",
        "Usually the numbers are floating point (to approximate better the real numbers) and hence the speed is measured in Floating Point Operations per Second (FLOPS)\n",
        "For example, the V100 GPU from Nvidia can do 15.7 Tera FLOPS of single precision arithmetic.\n",
        "\n",
        "Memory is the other important piece here. The data needs to be stored here and it needs to be streamed out to be fed into the MAC units (or execution cores). Memory within the same integrated circuit of GPU or CPU, allows for rapid streaming out and in of operands and results respectively. There is memory outside the CPU/GPU cores - in main memory (DRAMs), which are slower to slurp in from."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1T9jGeBbMeD"
      },
      "source": [
        "---\n",
        "**numpy**\n",
        "\n",
        "is a numerical matrix computation  library in *python*\n",
        "\n",
        "There are many such libraries in other programming languages.\n",
        "\n",
        "We will use the popular numerical linear algebra package numpy\n",
        "https://numpy.org/doc/stable/user/index.html\n",
        "The above website has documentation, tutorials etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fJZykwE8aNrm"
      },
      "outputs": [],
      "source": [
        "#import the package for numerical linear algebra computations in Python\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qq29Rdt7dzzA"
      },
      "source": [
        "---\n",
        "**Execution speed comparison of vectorised and non-vectorised code**\n",
        "We will generate a random vector with N entries and then use the timeit function to measure its execution time for two different implementations. A naive one using for loops and a vectorized one.\n",
        "\n",
        "*%%timeit -n num_loops -r num_runs*\n",
        "at the top of a cell, instruments it to measure execution time of the cell.\n",
        "\n",
        "The content of a cell are run back-to-back *num_loops* times each run and the execution time is measured. Thus the execution time of the code in the cell, measured for a single run:  $\\frac{execution\\_time}{num\\_loops}$.\n",
        "\n",
        "This repeated *num_run* times to collect execution times across different runs and then averaged and std_dev is obtained.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ufR99eBfZz6-"
      },
      "outputs": [],
      "source": [
        "\n",
        "N=10000\n",
        "x = np.random.rand(N,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9WDtELuajSf",
        "outputId": "0f077dca-01c8-4cbb-bb9d-4954a0325cec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.53 ms ± 126 µs per loop (mean ± std. dev. of 30 runs, 100 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 100 -r 30\n",
        "\n",
        "#find average\n",
        "sum = 0\n",
        "for i in range(len(x)):\n",
        "  sum = sum + x[i,0]\n",
        "\n",
        "avg = sum/N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJTDV2EbcIxK",
        "outputId": "43bcb70f-a7df-4cd1-971d-f1cd99572b85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15.2 µs ± 3.47 µs per loop (mean ± std. dev. of 30 runs, 100 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 100 -r 30\n",
        "avg = np.ones(N) @ x / N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZw2ywA5g1Ks",
        "outputId": "cdef9dc7-142d-4b53-e8c0-e7cbc90e117b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The slowest run took 4.75 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "6.48 µs ± 3.74 µs per loop (mean ± std. dev. of 30 runs, 100 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 100 -r 30\n",
        "#alternative using np.sum\n",
        "avg = np.sum(x)/N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "avg = np.mean(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Yt3wtC_e816",
        "outputId": "ac0dd91d-a95a-4543-cd24-7db4e38a0885"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.65 ms ± 68.2 µs per loop (mean ± std. dev. of 30 runs, 100 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 100 -r 30\n",
        "#find std deviation\n",
        "sum = 0\n",
        "for i in range(len(x)):\n",
        "  sum = sum + (x[i,0]-avg)**2\n",
        "\n",
        "std_dev = np.sqrt(sum/N)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvLlAAzifcvH"
      },
      "source": [
        "---\n",
        "**Q2: Vectorized implementation for calculating std dev**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81PvauCefbjL",
        "outputId": "7fbca8fa-6007-48e6-d3e6-2d7d6db52116"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The slowest run took 39.02 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "47.8 µs ± 121 µs per loop (mean ± std. dev. of 30 runs, 100 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 100 -r 30\n",
        "avg = np.sum(x)/N\n",
        "std_dev = np.sqrt(np.sum((x-avg)**2)/N)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSmKG9U0vC4s"
      },
      "source": [
        "---\n",
        "**Q3**\n",
        "\n",
        "Find the a) rank of A and b) the basis vectors of its null space, given that:\n",
        "\n",
        "$A = L U = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 3 & 1 & 0 & 0 \\\\ 4 & 1 & 3 & 0 \\\\ 6 & 4 & 3 & 2 \\end{bmatrix} \\begin{bmatrix} 1 & 3 & 0 & 1 & 2 & 1 \\\\ 0 & 0 & 3 & 4 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 3\\\\ 0 & 0 & 0 & 0 & 0 & 0 \\end{bmatrix}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWX8ibYyv62N"
      },
      "source": [
        "Ans:\n",
        "\n",
        "With $\\mathbf{A}_{m\\times n} = \\mathbf{L}_{m\\times m} \\mathbf{U}_{m\\times n}$, we see that $\\mathbf{U}$ should be the *row echelon form* of the matrix $\\mathbf{A}$.\n",
        "\n",
        "Thereby, we can see that $\\text{rank}(\\mathbf{A}) = \\text{rank}(\\mathbf{U})$, and since $\\mathbf{U}$ has three non-zero rows $\\implies \\boxed{\\text{rank}(\\mathbf{A}) = 3}$\n",
        "\n",
        "We can see that the $\\text{nullity}(\\mathbf{A}) = n - \\text{rank}(\\mathbf{A}) = 6 - 3 = 3$.\\\n",
        "The basis vectors of the matrix $\\mathbf{A}$ can be found by solving the equation $\\mathbf{A}\\mathbf{x} = \\mathbf{0}$, or equivalently using the matrix $\\mathbf{U}$ since $\\mathbf{L}$ is invertible here.\n",
        "\n",
        "\\begin{equation*}\n",
        "\\mathbf{U}\\mathbf{x} =\n",
        "\\begin{bmatrix}\n",
        "    1 & 3 & 0 & 1 & 2 & 1 \\\\\n",
        "    0 & 0 & 3 & 4 & 0 & 0 \\\\\n",
        "    0 & 0 & 0 & 0 & 0 & 3 \\\\\n",
        "    0 & 0 & 0 & 0 & 0 & 0\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "    x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "    0 \\\\ 0 \\\\ 0 \\\\ 0\n",
        "\\end{bmatrix}\n",
        "\\end{equation*}\n",
        "\n",
        "The free columns in $\\mathbf{U}$ are $\\{2, 4, 5\\}$, thereby the free variables are $x_2, x_4, x_5$.\n",
        "\n",
        "The system then is\n",
        "\n",
        "\\begin{equation*}\n",
        "\\begin{aligned}\n",
        "    x_1 + 3x_2 + x_4 + 2x_5 + x_6 &= 0 \\\\\n",
        "    3x_3 + 4x_4 &= 0 \\\\\n",
        "    3x_6 &= 0\n",
        "\\end{aligned}\n",
        "\\end{equation*}\n",
        "\n",
        "Writing the rest of the variables in terms of the free variables, we get\n",
        "\n",
        "\\begin{equation*}\n",
        "\\begin{aligned}\n",
        "    x_1 &= -3x_2 - x_4 - 2x_5 \\\\\n",
        "    x_2 &= x_2 \\\\\n",
        "    x_3 &= -\\frac{4}{3} x_4 \\\\\n",
        "    x_4 &= x_4 \\\\\n",
        "    x_5 &= x_5 \\\\\n",
        "    x_6 &= 0\n",
        "\\end{aligned}\n",
        "\\end{equation*}\n",
        "\n",
        "i.e., in matrix form\n",
        "\n",
        "\\begin{equation*}\n",
        "\\begin{bmatrix}\n",
        "    x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "    -3 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\n",
        "\\end{bmatrix} x_2\n",
        "+\n",
        "\\begin{bmatrix}\n",
        "    -1 \\\\ 0 \\\\ -\\frac{4}{3} \\\\ 1 \\\\ 0 \\\\ 0\n",
        "\\end{bmatrix} x_4\n",
        "+\n",
        "\\begin{bmatrix}\n",
        "    -2 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0\n",
        "\\end{bmatrix} x_5\n",
        "\\end{equation*}\n",
        "\n",
        "Therefore, the basis vectors of the null space of $\\mathbf{A}$ are\n",
        "\\begin{equation*}\n",
        "\\boxed{\n",
        "    \\begin{bmatrix} -3 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix},\n",
        "    \\begin{bmatrix} -1 \\\\ 0 \\\\ -\\frac{4}{3} \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix},\n",
        "    \\begin{bmatrix} -2 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}\n",
        "}\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJcL0vomhOln"
      },
      "source": [
        "---\n",
        "**Q4**\n",
        "Find all 2x2 matrices $A$ such that $A^2=I$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXW1SDw7jVv4"
      },
      "source": [
        "Ans:\n",
        "\n",
        "Let $A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}$\n",
        "\n",
        "$\\implies A^2 = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} = \\begin{bmatrix} a^2 + bc & ab + bd \\\\ ac + cd & bc + d^2 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$\n",
        "\n",
        "$\\implies a^2 + bc = d^2 + bc = 1$ and $ab + bd = ac + cd = 0$\n",
        "\n",
        "Case 1: $b = 0, \\ c = 0$\n",
        "\n",
        "$\\implies a^2 = d^2 = 1 \\implies a = \\pm 1, \\ d = \\pm 1$\n",
        "\n",
        "We have the solutions $\\Bigg\\{ \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}, \\begin{bmatrix} -1 & 0 \\\\ 0 & 1 \\end{bmatrix}, \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix}, \\begin{bmatrix} -1 & 0 \\\\ 0 & -1 \\end{bmatrix} \\Bigg\\}$.\n",
        "\n",
        "Case 2: $b = 0, \\ c \\neq 0$\n",
        "\n",
        "$\\implies a + d = 0$ and $a^2 = d^2$, we get $a = \\pm 1, d = \\mp 1$\n",
        "\n",
        "We have the solutions $\\Bigg\\{ \\begin{bmatrix} 1 & 0 \\\\ c & -1 \\end{bmatrix}, \\begin{bmatrix} -1 & 0 \\\\ c & 1 \\end{bmatrix} \\Bigg\\}$.\n",
        "\n",
        "Case 3: $b \\neq 0, \\ c = 0$\n",
        "\n",
        "$\\implies a + d = 0$ and $a^2 = d^2$, we get $a = \\pm 1, d = \\mp 1$\n",
        "\n",
        "We have the solutions $\\Bigg\\{ \\begin{bmatrix} 1 & b \\\\ 0 & -1 \\end{bmatrix}, \\begin{bmatrix} -1 & b \\\\ 0 & 1 \\end{bmatrix} \\Bigg\\}$.\n",
        "\n",
        "Case 4: $b \\neq 0, \\ c \\neq 0$\n",
        "\n",
        "$\\implies a + d = 0$ and $a^2 = d^2$, we get $a = \\pm (1 - bc), \\ ad= \\mp (1 - bc)$\n",
        "\n",
        "We have the solutions $\\Bigg\\{ \\begin{bmatrix} 1 - bc & b \\\\ c & -1 + bc \\end{bmatrix}, \\begin{bmatrix} -1 + bc & b \\\\ c & 1 - bc \\end{bmatrix} \\Bigg\\}$.\n",
        "\n",
        "Hence, we have the solutions $\\boxed{ \\Bigg\\{ \\begin{bmatrix} 1 & 0 \\\\ c & -1 \\end{bmatrix}, \\begin{bmatrix} -1 & 0 \\\\ c & 1 \\end{bmatrix}, \\begin{bmatrix} 1 & b \\\\ 0 & -1 \\end{bmatrix}, \\begin{bmatrix} -1 & b \\\\ 0 & 1 \\end{bmatrix}, \\begin{bmatrix} 1 - bc & b \\\\ c & -1 + bc \\end{bmatrix}, \\begin{bmatrix} -1 + bc & b \\\\ c & 1 - bc \\end{bmatrix} \\Bigg\\} }$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-ySUrR6PAWv"
      },
      "source": [
        "---\n",
        "**Symmetric and Skew-Symmetric Matrices**\n",
        "\n",
        "$A$ is a symmetric matrix if $A^T = A$\n",
        "\n",
        "if $A$ is symmetric and invertible, then  $A = LDU$ then $A^T = U^TDL^T = LDU$ and $L=U^T$ by uniques of factorization (see above). Therefore for symmetric matrices $A = LDL^T$\n",
        "\n",
        "$A$ is skew symmetric if $A^T = -A$\n",
        "Note that the diagonal values of a skew symmetric matrix are all zeros.\n",
        "\n",
        "Example: $\\begin{bmatrix} 0 & 3 & 2 \\\\ -3 & 0 & 1 \\\\ -2 & -1 & 0 \\end{bmatrix}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVBDjuxFn-qs"
      },
      "source": [
        "---\n",
        "**Vector product**\n",
        "\n",
        "In 3D, we are used to vectors represented as $\\overrightarrow{v} = v_1 \\overrightarrow{i} + v_2 \\overrightarrow{j} + v_3 \\overrightarrow{k}$ and $\\overrightarrow{w} = w_1 \\overrightarrow{i} + w_2\\overrightarrow{j} + w_3 \\overrightarrow{k}$\n",
        "\n",
        "We have the notion of cross product of two vectors, which gives a third vector which is normal to both. Note that any  two vectors define a plane and the cross product is a vector normal to the plane.\n",
        "\n",
        "The cross product is defined as $\\overrightarrow{w} \\times \\overrightarrow{v} = (w_2v_3 - w_3v_2)\\overrightarrow{i} + (w_3v_1 - w_1v_3) \\overrightarrow{j} + (w_1v_2 - w_2v_1) \\overrightarrow{k}$\n",
        "\n",
        "\n",
        "We can calculate vector product through matrix vector multiplication, if we represent $\\overrightarrow{w}$ as a skew symmetric matrix as: $[w]_\\times = \\begin{bmatrix} 0 & -w_3 & w_2 \\\\ w_3 & 0 & -w_1 \\\\ -w_2 & w_1 & 0 \\end{bmatrix}$\n",
        "\n",
        "Now $\\omega \\times v = [w]_\\times v = \\begin{bmatrix} 0 & -w_3 & w_2 \\\\ w_3 & 0 & -w_1 \\\\ -w_2 & w_1 & 0 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{bmatrix}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-c96MjURWhW"
      },
      "source": [
        "---\n",
        "**Q6 Vector product**\n",
        "\n",
        "a) What is $v \\cdot \\omega \\times v$\n",
        "\n",
        "b) What is $\\omega \\times \\omega$\n",
        "\n",
        "\n",
        "c) What is the rank of $[\\omega]_\\times$ (*Hint*: You can guess from its operation of realizing the cross product)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp2jOZVzeb5c"
      },
      "source": [
        "Ans:\n",
        "\n",
        "(a).\n",
        "The right way to interpret $\\mathbf{v} \\cdot \\mathbf{w} \\times \\mathbf{v}$ is as $\\mathbf{v} \\cdot (\\mathbf{w} \\times  \\mathbf{v})$, which evaluates to 0.\n",
        "\n",
        "The vector $\\mathbf{w} \\times \\mathbf{v}$ is orthogonal to $\\mathbf{v}$ and since the dot product between any two orthogonal vectors is zero, we have the result.\n",
        "\n",
        "This is a degenerate case of the scalar triple product with two coinciding vectors.\n",
        "The scalar triple product $\\mathbf{a} \\cdot (\\mathbf{b} \\times \\mathbf{c})$ is the volume of the parallelepiped formed by the three vectors $\\mathbf{a}$, $\\mathbf{b}$, and $\\mathbf{c}$.\n",
        "If two of the vectors are the same, the volume of the parallelepiped is 0.\n",
        "\n",
        "(b).\n",
        "The cross product of a vector with itself is always the zero vector, so $\\omega \\times \\omega = \\mathbf{0}$.\n",
        "\n",
        "This is because the cross product of two vectors is orthogonal to both vectors, and the only vector that is orthogonal to itself is the zero vector.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLot_90rpnME"
      },
      "source": [
        "---\n",
        "**Matrix Product and Matrix Group**\n",
        "\n",
        "Associativity: $A(BC) = (AB)C$ provided the multiplication is possible in terms of the matrix dimensions.\n",
        "\n",
        "Not commutative: $AB \\ne BA$ in general (even when A & B are square)\n",
        "\n",
        "Consider the set of $nxn$ non-singular matrices $GL(n) = \\{A \\in ℜ^{nxn}\\}$ along with the matrix product '·'. This is a \"Group\" and called the General linear group:\n",
        "\n",
        "1. Closure under multipication: $A, B \\in GL(n)$ then $A·B \\in GL(n)$\n",
        "2. Associative: $A, B, C \\in GL(n)$ then $(A·B)·C = A·(B · C)$\n",
        "3. Identity: $I_n \\in GL(n)$ such that $A · I_n = I_n · A = A$\n",
        "4. Inverse: $∀\\ A \\in GL(n)$ then $∃\\ A^{-1}$ such that $A · A^{-1} =I_n = A^{-1}A$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCpRXl6ptiVp"
      },
      "source": [
        "---\n",
        "**Q7**\n",
        "\n",
        "Which of these matrix sets are groups under usual matrix multiplication and Inverse?\n",
        "\n",
        "a) Set of symmetric matrices\n",
        "\n",
        "b) Set of Positive Matrices (all entries are strictly greater than 0)\n",
        "\n",
        "c) Set of diagonal invertible matrices\n",
        "\n",
        "d) Set of permutation matrices (Permutation matrix has exactly single 1 in each row and column and zeros every where else)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a987ThzOu_0Q"
      },
      "source": [
        "Ans:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn8gZe-K8F0D"
      },
      "source": [
        "---\n",
        "**A Brief on Number Representations**\n",
        "\n",
        "Floating point numbers are presented as below (https://en.wikipedia.org/wiki/Floating-point_arithmetic):\n",
        "\n",
        "Type | Total Bits | sign | exponent | mantissa | bits of precision | exponent bias  | decimal digits(mantissa)\n",
        "-----|------------|------|----------|----------|-------------------|----------|------------\n",
        "Half Precision | 16 | 1 | 5 | 10 | 11 | 15 | ~3.3\n",
        "Single Precision|32 | 1 | 8 | 23 | 24 |127 | ~7.2\n",
        "Double Precision|64 | 1 | 11| 52 | 53 |1023|~15.9\n",
        "Extended Precision (x86)|80| 1| 15| 64 | 64 |16383|~19.2\n",
        "\n",
        "Lets take 16-bit numbers as an example.\n",
        "the left most bit: is the sign bit.\n",
        "Next 10 bits are the mantissa (or significand).\n",
        "Last 5 bits are exponent.\n",
        "\n",
        "Normalized numbers have a binarypoint after the first bit and the first bit is always 1. For example, a binary number 0.001x$2^0$ is represented as 1.000 x $2^{-3}$. That is, you left shift till the first non-zero bit comes to the first location (and corresponding adjust the exponent).\n",
        "\n",
        "Since the first bit is '1' for a normalized representation, we dont need to store it. Hence we effectively have 10+1 = 11 bits of precision. Precision refers to the accuracy of representation. The gap between two numbers representatble by the mantissa is $2^{-10}$ and the round-off error is half of that and is $2^{-11}$\n",
        "\n",
        "The exponent takes values from 00000 (0) to 11111 (31). All 1's is used to represent infitinity and NaN and all 0 for sub-normal numbers (where the first bit is not 1) Hence the exponent ranges from 1 to 30. With exponent bias of 15, the represented exponent is subtracted by the bias  to give the actual exponent. for example, an exponent if -11 is represented as -11+15 = 4. Hence the representable exponents range from -14 to 15 (30 values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaK-NJ9K6Fln",
        "outputId": "26504723-9a5d-4148-96c8-e984d0e11022"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "float16 info: Machine parameters for float16\n",
            "---------------------------------------------------------------\n",
            "precision =   3   resolution = 1.00040e-03\n",
            "machep =    -10   eps =        9.76562e-04\n",
            "negep =     -11   epsneg =     4.88281e-04\n",
            "minexp =    -14   tiny =       6.10352e-05\n",
            "maxexp =     16   max =        6.55040e+04\n",
            "nexp =        5   min =        -max\n",
            "smallest_normal = 6.10352e-05   smallest_subnormal = 5.96046e-08\n",
            "---------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"float16 info:\",np.finfo(np.float16) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWzrzcSu8v6A"
      },
      "source": [
        "**Round off errors**\n",
        "\n",
        "Since the floating point representation can only represent finite numbers of values on the real line, there will be round off error of about $2^{-p}/2$.\n",
        "\n",
        "As an example below, the real number 0.1 can only be represented within $0.02\\%$ with 16-bit floating point numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmbHGo3O_r4d",
        "outputId": "1f325d05-173f-4432-ae7b-de2be5ffb580"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Percentage error in value stored in f16=-0.02441406250000555%, and occupies 2 bytes\n",
            "Error in value stored in f32=1.4901161138336505e-06%, and occupies 4 bytes\n",
            "Error in value stored in f64=0.0%, and occupies 8 bytes\n"
          ]
        }
      ],
      "source": [
        "#declare a variable to be half precision floating point type and print its characteristics\n",
        "f16 = np.float16(\"0.1\")\n",
        "f32 = np.float32(\"0.1\")\n",
        "f64 = np.float64(\"0.1\")\n",
        "print(f\"Percentage error in value stored in f16={(f16-0.1)*100/0.1}%, and occupies {f16.itemsize} bytes\")\n",
        "print(f\"Error in value stored in f32={(f32-0.1)*100/0.1}%, and occupies {f32.itemsize} bytes\")\n",
        "print(f\"Error in value stored in f64={(f64-0.1)*100/0.1}%, and occupies {f64.itemsize} bytes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTuRFIUO9z3h"
      },
      "source": [
        "---\n",
        "**Well Conditioned and Ill-Conditioned**\n",
        "\n",
        "Regardless of the round off error, the nature of the matrix (or in general a function) will be such that it is very sensitive to inputs.\n",
        "\n",
        "For example, in solving $Ax=b$, the input is $b$ and output $x$. Depending on the condition number of the matrix, a small perturbation of $b$ can lead to a large change in the solution $x$.\n",
        "\n",
        "The example below illustrates this. We perturb $b$ to $d$ and $e$ and look at the changes in the soliutions.\n",
        "\n",
        "The ratio of the output change (i.e. change in solution) to input change (change in b) ) is roughly equal to the condition number (kind of like an amplifier gain)\n",
        "\n",
        "Too high a condition number means a ill-conditioned system. Any small errors in the input will through the solution off.\n",
        "\n",
        "The condition number of a matrix is related the ratio of its largest singular value to the smallest singular value (which in turn is related to the eigen values for a square matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3nV0-stvntG",
        "outputId": "4e5adbea-e070-4217-fe30-59eb0e75616d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x=[2. 0.], y=[-7.9976163  9.997616 ], cond(A)=39995.3671875\n",
            "||b-d|| = 0.0009999275207519531, ||x-y||=14.138764381408691\n",
            "||b-e|| = 0.009000062942504883, ||x-z||=127.25898742675781\n"
          ]
        }
      ],
      "source": [
        "A = np.array([[1,1],[1,1.0001]],np.float32)\n",
        "b = np.array([2,2],np.float32)\n",
        "d = np.array([2.0,2.001],np.float32)\n",
        "e = np.array([2.0,2.009],np.float32)\n",
        "\n",
        "x = np.linalg.solve(A,b)\n",
        "y = np.linalg.solve(A,d)\n",
        "z = np.linalg.solve(A,e)\n",
        "\n",
        "print(f\"x={x}, y={y}, cond(A)={np.linalg.cond(A,p=2)}\")\n",
        "print(f\"||b-d|| = {np.linalg.norm(b-d)}, ||x-y||={np.linalg.norm(x-y) }\")\n",
        "print(f\"||b-e|| = {np.linalg.norm(b-e)}, ||x-z||={np.linalg.norm(x-z) }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBd6367n_NZs"
      },
      "source": [
        "---\n",
        "**Orthogonality**\n",
        "\n",
        "Two vectors are orthogonal if their inner product (or dot product) is 0.\n",
        "\n",
        "If $v_1, v_2, ... , v_k$ are pairwise orthogonal then they are linearly independent.\n",
        "\n",
        "To see this, let $c_1 v_1 + c_2 v_2 + .... c_k v_k = 0$, then taking the dot product with $v_j$ on both sides gives $c_j v_j \\cdotp v_j = 0$ and hence $c_j=0$ for all j.\n",
        "\n",
        "\n",
        "*Orthogonality of Row space with NullSpace*\n",
        "\n",
        "Let $A: V_n → W_m$ . Then\n",
        "\n",
        "Null Space $N(A) = \\{v \\in V_n: Av = 0\\} $\n",
        "\n",
        "Row Space $R(A) = \\{v \\in V_n: s.t.\\ \\ v^T = w^T A, w \\in W_m\\}$\n",
        "\n",
        "Then if $x \\in R(A)$ then $x^T = u^TA$ for some $u \\in W_m$\n",
        "Let $y  \\in N(A)$ then\n",
        "\n",
        "$x^Ty = (u^TA)y = u^T(Ay) = 0$\n",
        "\n",
        "Thus $R(A) \\perp N(A)$ and each is called the orthogonal complement of the other.\n",
        "\n",
        "\n",
        "Example:\n",
        "\n",
        "$A = \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix}$ has rank 1, which is the rank of the $RowSpace(A)$.\n",
        "\n",
        "$N(A) = \\{\\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}\\}$ such that $\\begin{bmatrix} x & y & z \\end{bmatrix}\\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix} = ax + by +cz = 0\\}$ is the orthogonal 2D plane."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3qgP2tAr7xm"
      },
      "source": [
        "---\n",
        "We know that column space of $A$ is $C(A) = \\{w = Av\\ \\forall\\ v \\in V_n\\}$\n",
        "\n",
        "$dim(C(A)) = dim(R(A)) = rank(A)$\n",
        "\n",
        "We can define left null space of $A$ as $LN(A) = \\{w | w^TA = 0\\}$\n",
        "\n",
        "Note that $LN(A) \\perp C(A)$\n",
        "\n",
        "This leads to the nice diagram of the front cover of the Strang textbook.\n",
        "\n",
        "Note that we can do a one-to-one map of $R(A)$ with $C(A)$ as:\n",
        "\n",
        "$A:R(A) → C(A)$ with $x_r \\in R(A)$ and $Ax_r = b \\in C(A)$\n",
        "\n",
        "This is a one-to-one mapping as otherwise if $Ax_r^{'} = b$, then $A(x_r^{'}-x_r) = 0$ and $x_r^{'}-x_r \\in N(A)$.\n",
        "\n",
        "However since $R(A)$ is a vector subspace, $x_r^{'}-x_r \\in R(A)$ which can only happen if there difference is 0 and hence they are equal.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUTzrI--r0oV"
      },
      "source": [
        "---\n",
        "**Least Squares Solution**\n",
        "Let $A:V_n → W_m$ and $rank(A) = k \\leq min(m,n)$\n",
        "\n",
        "Then $A^TA$ is a $nxn$ matrix and $AA^T$is a $mxm$ matrix, each with rank $k$.\n",
        "\n",
        "Both are symmetric.\n",
        "\n",
        "Lets assume that $m > n$ and $rank(A) = n$, that is more rows than columns.\n",
        "\n",
        "Then $Ax =b $ need not have any solution as $b$ can be outside the the columns space of A (which has a dimension only $n$.\n",
        "\n",
        "This is a common problem in many data fitting problems.\n",
        "\n",
        "*Example*\n",
        "\n",
        "Imagine that we have $m$ observations $\\{(a_i,b_i\\}$ and we want to fit a polynomial equation of degree $n-1$ to fit this. i.e\n",
        "\n",
        "$y = \\sum_{k=0}^{n-1}z_k x^k$\n",
        "\n",
        "where we want to find the best estimates of the unknown parameters $\\{z_k\\}$\n",
        "\n",
        "using the observed data, we can write $m$ simultaneous equations\n",
        "\n",
        "$\\sum_{k=0}^{n-1}z_k a_i^k = b_i$ one per observation $(a_i, b_i)$\n",
        "\n",
        "this can be rewritten as a matrix equaiton:\n",
        "\n",
        "$Az = b$ where $A$ is a $mxn$ matrix and\n",
        "\n",
        "$A = \\begin{bmatrix} 1 & a_1 & a_1^2 & ...& a_1^{n-1} \\\\ 1 & a_2 & a_2^2 & ... & a_2^{n-1} \\\\ ... \\\\ 1 & a_m & a_m^2 & ... & a_m^{n-1}\\end{bmatrix}$ and $b=\\begin{bmatrix} b_1 \\\\ b_2 \\\\ ... \\\\ b_m \\end{bmatrix}$\n",
        "\n",
        "\n",
        "**NOTE** the equations are linear in the unknowns and can be non-linear in the data terms.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKWcdLvM1riB"
      },
      "source": [
        "---\n",
        "**Geometric Solution to Least Squares**\n",
        "\n",
        "We want to find $x^{'}$ such that error $e=b-Ax^{'}$ is minimum.\n",
        "\n",
        "From geometry, we know that this happens when error is perpendicular to column space, i.e. $e \\perp C(A)$. Hence $e \\in LN(A)$ is in left nullspace of $A$.\n",
        "\n",
        "Hence $A^Te = A^T(b-Ax^{'}) = 0$ which can be written as\n",
        "\n",
        "$A^TAx^{'} = A^Tb$ and hence $x^{'} = (A^TA)^{-1}A^Tb$ is the solution such that $Ax^{'} = A(A^TA)^{-1}A^Tb$ is closest to b.\n",
        "\n",
        "Since we asumed rank(A) = n, rank($A^TA$) = n and it is invertible.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89dU4hRm6SaN"
      },
      "source": [
        "---\n",
        "**Projection Matrix**\n",
        "\n",
        "The matrix $P = A(A^TA)^{-1}A^T$ is called the projection matrix and it projects $b$ to $C(A)$\n",
        "\n",
        "Note that $P^2 = P$ (check).\n",
        "\n",
        "rank(P) = n\n",
        "\n",
        "\n",
        "In general, if we are given $\\{v_1, .... v_k\\}$ and want to project another vector $w$ onto the Span($\\{v_1, .... v_k\\}$), we can form a matrix\n",
        "$A = [v_1 v_2 ... v_k]$ and find the projection matrix $P=A(A^TA)^{-1}A^T$ and do $P=Pw$ to do the projection.\n",
        "\n",
        "Hence, in general if $S \\subset V$ a subspace and $P_S$ be the projection matrix that projects any vector onto the subspace, then\n",
        "\n",
        "for any $v \\in V$ we can write\n",
        "\n",
        "$v_S = P_S v$ and $v_\\perp = v - P_S v = (I - P_S)v$ with $v = v_S + v_\\perp$\n",
        "\n",
        "Therefore a reflection of a vector $v$ about a subspace can be written as:\n",
        "\n",
        "$v_r = v_\\perp - v_S = (I-2P_S)v$\n",
        "\n",
        "Hence $H=I-2P_S$ is the reflection matrix about a subspace $S$. This is like a ray of light reflecting of the surface\n",
        "\n",
        "Note -$-v_r$ is also another kind of reflection (mirror reflection)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVHPD5Nv7Skl"
      },
      "source": [
        "---\n",
        "**Optimization based derivation of least means square solution**\n",
        "\n",
        "The residual error is $e = b-Ax$ and we would like to minimize its $L_2$ norm as:\n",
        "\n",
        "$min_x ||e||^2$ where\n",
        "\n",
        "$||e||^2 = e^Te  = (b-Ax)^T (b-Ax) = x^TA^TAx -b^TAx - x^TA^Tb + b^Tb$\n",
        "\n",
        "Note that $b^TAx = x^TA^Tb$ as it is a scalar\n",
        "\n",
        "We have $n$ elements in the variable $x$ and we should differentiate with resepect to each and put the derivative to 0.\n",
        "\n",
        "This can be compactly done using vector differentiation or gradient as:\n",
        "\n",
        "$\\nabla L = 2A^TAx-2A^Tb = 0$ (at extrema. we have used L to represent the square error). The extrema point $x^{'} = (A^TA)^{-1}A^Tb$ same as derived via geometric consideration.\n",
        "\n",
        "The equivalent for double derivative is called the Hessian and is $A^TA$ which is positive semi definite in general and positive definite for vectors not in the null space of A (equivalent to second derivative being positive) and hence the extrema is a minima.\n",
        "\n",
        "A matrix $B$ is positive definite is $x^TBx > 0 $ for all $x \\neq 0$.\n",
        "\n",
        "Since $x^T(A^TA)x = (x^TA^T)(Ax) = ||Ax||^2$ it is positive definite. I.e. the shape of the loss function at this point is \"upward facing parabola\" as anydirection you move, will only increase L.\n",
        "\n",
        "\n",
        "The benefit of this optimization approach is that we can add *regularization*, ie additional constraints on the parameters. for example we can look for parameters that are not too large in magnitude. i.e. $||x||^2$ is kept small. Or we might also bring in weights for different error terms via a diagonal weight matrix $e = W(b-Ax)$\n",
        "\n",
        "So $L = e^Te + \\lambda x^Tx$ is the loss function to be minimized and\n",
        "\n",
        "$\\nabla L = (A^TW^TWA+\\lambda I)x^{'}-A^TW^TWb=0$ and hence\n",
        "\n",
        "$x^{'} = (A^TW^TWA+\\lambda I)^{-1}A^TW^TWb$\n",
        "\n",
        "is the optimal solution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU2edhEjLAfu"
      },
      "source": [
        "---\n",
        "**Q7**\n",
        "\n",
        "Find the weighted least squares solution to $A = \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 1 & 2 \\end{bmatrix}$ $b = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix}$ and $W=\\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRG2Vn5IMZFi",
        "outputId": "0f5612ad-e669-43e4-b5e5-86fc40151bb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x=[0.04761905 0.57142857], Ax = [0.04761905 0.61904762 1.19047619],\n",
            " e = [ 0.04761905 -0.38095238  0.19047619] and ||e||=0.6546536707079771\n"
          ]
        }
      ],
      "source": [
        "#Ans\n",
        "\n",
        "A = np.array([[1., 0],[1,1],[1,2]])\n",
        "b = np.array([0,1,1])\n",
        "W = np.array([[2,0,0],[0,1,0],[0,0,1]])\n",
        "\n",
        "#Your Answer Here\n",
        "x = np.linalg.inv(A.T @ W.T @ W @ A) @ A.T @ W.T @ W @ b\n",
        "e = A@x - b\n",
        "print(f\"x={x}, Ax = {A@x},\\n e = {e} and ||e||={np.sqrt(np.linalg.norm(e))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70Gu85QfObnI"
      },
      "source": [
        "---\n",
        "**Q8**\n",
        "If $V = Span(\\{[1,1,0,1]^T, [0,0,1,0]^T\\})$, find\n",
        "\n",
        "a) a basis for orthogonal complement of $V$ which is $V^\\perp$\n",
        "\n",
        "b) The projection matrix $P$ onto V\n",
        "\n",
        "c) The vector in $V$ closest to $b=[0,1,0,-1]$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15Xe-ve3PCl4"
      },
      "source": [
        "Ans:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZXCg-diQ_cO",
        "outputId": "d0dcb2c7-b7df-4507-c1fc-745860c1168b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P=[[0.33333333 0.33333333 0.         0.33333333]\n",
            " [0.33333333 0.33333333 0.         0.33333333]\n",
            " [0.         0.         1.         0.        ]\n",
            " [0.33333333 0.33333333 0.         0.33333333]] and closest vector in V to b =[0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "#Ans contd.\n",
        "A=np.array([[1,0],[1,0],[0,1],[1,0]])\n",
        "b = np.array([0,1,0,-1])\n",
        "\n",
        "#Your code here\n",
        "P = <your code>\n",
        "Pb = <your code>\n",
        "\n",
        "print(f\"P={P} and closest vector in V to b ={<your code>}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ayxa88H9Sn1F"
      },
      "source": [
        "---\n",
        "**Q9**\n",
        "\n",
        "Let $P$ Project onto Subspace $S$ and $Q$ project onto its orthogonal complement $S^\\perp$.\n",
        "\n",
        "a) What is $P+Q$\n",
        "\n",
        "b) What is $PQ$\n",
        "\n",
        "c) Show that $P-Q$ is its own inverse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uwgy9HveS-DC"
      },
      "source": [
        "Ans:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYh5vMaxUZFz"
      },
      "source": [
        "---\n",
        "**Q10**\n",
        "\n",
        "If $P=P^TP$ show that it is a projection matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTt1B70qUf9s"
      },
      "source": [
        "Ans:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lK0MHZcU5eo"
      },
      "source": [
        "---\n",
        "**Q11**\n",
        "\n",
        "Let $v_1 = [1,1,0]^T$ and $v_2=[1,1,1]$ be two vectors that span a plane.\n",
        "\n",
        "a) Find the projection matrix $P$ that will project any vector onto that plane\n",
        "\n",
        "b) Find a non-zero vector which projects to 0 when P is applied."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1cSndINVtaV",
        "outputId": "4629c822-91c6-4a30-b312-0baf35824c8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P=[[0.5 0.5 0. ]\n",
            " [0.5 0.5 0. ]\n",
            " [0.  0.  1. ]] b=[ 1 -1  0]\n"
          ]
        }
      ],
      "source": [
        "#Ans\n",
        "\n",
        "A = np.array([[1,1],[1,1],[0,1]])\n",
        "P = <your code>\n",
        "b = <your code>\n",
        "print(f\"P={P} b={b}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQogQ4bjKAVf"
      },
      "source": [
        "---\n",
        "**Orthogonal Matrices**\n",
        "\n",
        "A set of vectors is orthonormal if they are pairwise orthogonal and their norm is unity.\n",
        "\n",
        "Any matrix $A$ which is $m x n$ and $m \\geq n$ can be decomposed in to a product of a matrix $Q$ which is $m x n$ with orthonormal columns and an upper triangular matrix $R$ which is $n x n$\n",
        "\n",
        "By definition $Q^TQ = I$, a $nxn$ identity matrix.\n",
        "\n",
        "*Orthogonal matrix* is a square matrix with orthonormal columns.\n",
        "\n",
        "*Orthogonal matrix preserves vector lenghts $(L_2) and angles$*\n",
        "\n",
        "$||Qx||^2 = x^TQ^TQx = x^Tx = ||x||^2$\n",
        "\n",
        "$<Qx,Qy> = x^TQ^TQy = x^Ty = <x,y>$\n",
        "\n",
        "Any matrix which preserves angles and lenghts is orthogonal. (You can prove by considering its actions on unit orthogonal vectors)\n",
        "\n",
        "Example:\n",
        "\n",
        "$Q(\\theta) = \\begin{bmatrix} \\cos \\theta & -\\sin \\theta \\\\ \\sin \\theta & \\cos \\theta \\end{bmatrix}$\n",
        "\n",
        "rotates vectors in 2D counter clockwise by $\\theta$\n",
        "\n",
        "$Q^T = Q(-\\theta) = Q^{-1}$ does a rotation clock wise\n",
        "\n",
        "Example:\n",
        "\n",
        "Permutation matrices also preserve lengths and are orthogonal matrices.\n",
        "\n",
        "\n",
        "*Uniqueness of QR factorization**\n",
        "\n",
        "Lets assume that $A$ is non-singular i.e. rank n.\n",
        "\n",
        "Then $A=QR = QDDR$ where $D^2 = I$ the identity matrix. Each element of D is +1 or -1, hence in general $QR$ factorization is not unique. However if we insist on diagonals of $R$ be positive, then factorization is unique.\n",
        "\n",
        "If $rank(A) = k < n$ then factorisation is not unique. Let first $k$ columns of $A$ be linearly independent, then $A = \\begin{bmatrix}q_1 & q_2 &.. &q_k &q_{k+1} &.. &q_n\\end{bmatrix} \\begin{bmatrix} r_{11} &  r_{12} & ..& r_{1k}& r_{1{k+1}}& ..& r_{1n} \\\\ 0 & r_{21} & .. & r_{2k} & r_{2{k+1}} & .. & r_{2n} \\\\ ... \\\\ 0 & 0 & .. & r_{kk} & r_{k{k+1}} & .. & r_{kn} \\\\ 0 & 0 & .. & 0 & 0 & .. & 0 \\\\ ...\\end{bmatrix}$ and $q_{k+1}, ... q_n$ can be any orthogonal basis of null space of $A$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyMRN25aNTKk"
      },
      "source": [
        "---\n",
        "**Gram-Schmidt and QR decomposition**\n",
        "\n",
        "Let $A = [a_1, a_2, ..., a_n]$ be a mxn matrix with $m \\leq n$\n",
        "\n",
        "Then Gram-Schmidt procedure for orthogonalization is:\n",
        "\n",
        "$b_1 = a_1$ and $q_1 = \\frac{1}{||b_1||} b_1$\n",
        "\n",
        "$b_2 = a_2 - <a_2, q_1>q_1$ and $q_2 = \\frac{1}{||b_2||} b_2$\n",
        "\n",
        "$b_3 = a_3 - <a_3, q_2>q_2 -<a_3, q_1>q_1$ and $q_3 = \\frac{1}{||b_3||} b_3$\n",
        "\n",
        "and so on.\n",
        "\n",
        "We can rearrange and write:\n",
        "\n",
        "$A = [a_1, a_2, ..., a_n] = [q_1, q_2,  ... q_n]\\begin{bmatrix} q_1^Ta_1 & q_1^Ta_2 & ... & q_n^Ta_n \\\\ 0 & q_2^Ta_2 & ... & q_n^Ta_n \\\\ ... \\\\ 0 & 0 & ... & q_n^Ta_n \\end{bmatrix} = QR$\n",
        "\n",
        "$Q$ has orthonormal columns and if $m = n$, Q is an orthogonal matrix.\n",
        "\n",
        "Solving least squares becomes easy as ⁉\n",
        "\n",
        "$Ax^{'} = QRx^{'} = b$ and $Rx^{'} = Q^Tb$ can be solved by backward substitution\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu_dtdNcTy4q"
      },
      "source": [
        "---\n",
        "**Q12**\n",
        "\n",
        "If $u$ is a unit vector, then show that $Q=I-2uu^T$ is a orthogonal matrix. (it is infact the reflection about the vector $u$)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCoD1I07cEmi"
      },
      "source": [
        "Ans:\n",
        "\n",
        "With $\\mathbf{Q} = \\mathbf{I} - 2\\mathbf{u}\\mathbf{u^T}$, we can see that for it's transpose,\n",
        "\n",
        "$\n",
        "\\mathbf{Q^T}\n",
        "= (\\mathbf{I} - 2\\mathbf{u}\\mathbf{u^T})^\\mathbf{T}\n",
        "= \\mathbf{I^T} - 2(\\mathbf{u}\\mathbf{u^T})^\\mathbf{T}\n",
        "= \\mathbf{I^T} - 2((\\mathbf{u^T})^\\mathbf{T}(\\mathbf{u})^{\\mathbf{T}})\n",
        "= \\mathbf{I} - 2\\mathbf{u}\\mathbf{u^T}\n",
        "= \\mathbf{Q}\n",
        "$\n",
        "\n",
        "Hence, $\\mathbf{Q}$ is orthogonal.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agzX3gTXdQN5"
      },
      "source": [
        "---\n",
        "**Q13**\n",
        "\n",
        "Let $A = \\begin{bmatrix} 1 & 1\\\\2 & -1 \\\\ -2 & 4\\end{bmatrix}$ and $b=[1, 2, 7]^T$\n",
        "\n",
        "\n",
        "a) find orthonormal $q_1, q_2, q_3$ such that the first two span the column space of $A$\n",
        "\n",
        "b) Which fundamental subspace, defined by $A$, does $q_3$ belong to?\n",
        "\n",
        "c) Use QR decomposition to solve $Ax=b$ with least squares solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnwYlAQLerh1"
      },
      "source": [
        "Ans:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msUucrnweIvn",
        "outputId": "b4bf7c8d-354c-42b1-c23a-0bc193f741ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q=[[-0.33333333 -0.66666667]\n",
            " [-0.66666667 -0.33333333]\n",
            " [ 0.66666667 -0.66666667]],\n",
            " R=[[-3.  3.]\n",
            " [ 0. -3.]]\n",
            "Solution x=[1. 2.]\n"
          ]
        }
      ],
      "source": [
        "#Ans:\n",
        "\n",
        "A = np.array([[1,1],[2,-1],[-2,4]])\n",
        "\n",
        "Q,R = <your code>\n",
        "\n",
        "print(f\"Q={Q},\\n R={R}\")\n",
        "\n",
        "b = np.array([1,2,7])\n",
        "c = <your code>\n",
        "x = <your code>\n",
        "print(f\"Solution x={x}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcR9PBXXfmn3"
      },
      "source": [
        "---\n",
        "**Householder tranformations to obtain QR decomposition**\n",
        "\n",
        "We know that if $u$ is a vector than $H=I-2uu^T$ is a reflection matrix such that $Hv$ reflects $v$ about $u$ and $H$ is orthogonal.\n",
        "\n",
        "So idea is to take $A$ and work column by column by multiplying with suitable $H_1, H_2,...$ such that it gets converted into $R$ which is upper triangular.\n",
        "\n",
        "i.e. $H_n H_{n-1} ... H_1 A = R$\n",
        "\n",
        "Then $Q^T = H_n H_{n-1} ... H_1$\n",
        "\n",
        "Consider what matrix can reflect $x$ to $y$.\n",
        "\n",
        "We can find $u = x-y$ and then $u_n = \\frac{u}{||u||}$ is the unit normal then $H_u = I-2u_nu_n^T$ will move $x$ to $y$.\n",
        "\n",
        "This idea is used step by step. Start with first column of $A$ which is  $a_1$. We want to move it to $\\begin{bmatrix} ||a_1|| \\\\ 0 \\\\ ... \\\\0 \\end{bmatrix}$ by finding the suitable householder transformation $H_1$, which we can find by the idea in previous para. (Note the reason for the choice of the first element is to preserve the length of $a_1$ as orthogonal matrices preserve that.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
