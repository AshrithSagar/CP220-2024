{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AshrithSagar/CP220-M4RAS-2024/blob/main/3_hw_cp220_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt9kfthZOWZr"
      },
      "source": [
        "**Lab 3**\n",
        "\n",
        "*Topics*\n",
        "\n",
        "Gaussian Elimination <br>\n",
        "Inverse & Non-Singular <br>\n",
        "LU Factorization <br>\n",
        "Least Squares <br>\n",
        "Number representation <br>\n",
        "Orthogonality, Gram-schmidt, Householder <br>\n",
        "Projection Matrices <br>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ5K9GvLqJkV"
      },
      "source": [
        "---\n",
        "**Q1**\n",
        "\n",
        "Let $A=\\begin{bmatrix} a_{:1} & a_{:2} & .. & a_{:n} \\end {bmatrix}$ be a $mxn$ matrix and $B = \\begin{bmatrix} b_{:1}^T \\\\ b_{:2}^T \\\\ .. \\\\ b_{:n}^T \\end{bmatrix}$ be a $nxk$ matrix.\n",
        "\n",
        "Express $AB$ in terms of the products of columns of A and rows of B. What is its dimensions\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-yu1BmylGnD"
      },
      "source": [
        "Ans:\n",
        "\n",
        "$AB = \\sum_{i=1}^n a_{:i}b_{:i}^T$ and is a $mxk$ matrix\n",
        "\n",
        "In order to see this, let $A:U_n → V_m$ and $B: W_k → V_m$\n",
        "\n",
        "Then $AB: W_k → V_m$\n",
        "\n",
        "Let $w \\in W_k$ and $w = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ .. \\\\w_k \\end{bmatrix}$\n",
        "\n",
        "$Bw = \\begin{bmatrix} b_{:1}^T w \\\\ b_{:2}^T w \\\\ .. \\\\ b_{:n}^T w \\end{bmatrix}$\n",
        "\n",
        "Therefore $ABw = A\\begin{bmatrix} b_{:1}^T w \\\\ b_{:2}^T w \\\\ .. \\\\ b_{:n}^T w \\end{bmatrix} = (b_{:1}^T w) a_{:1} + (b_{:2}^T w)a_{:2} + .. + (b_{:n}^T w) a_{:n}$\n",
        "\n",
        "In the right side - we are adding scalar multiples of all columns of A. Note that for any scalar $\\lambda$, $\\lambda \\begin{bmatrix} a_1 \\\\ a_2 \\\\ ... \\\\ a_m \\end{bmatrix} = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ ... \\\\ a_m \\end{bmatrix} \\lambda$\n",
        "\n",
        "Hence we can rewrite\n",
        "$ABw = a_{:1} (b_{:1}^T w) + a_{:2} (b_{:2}^T w) + ... = (a_{:1}b_{:1}^T) w + (a_{:2}b_{:2}^T)w + ... $ by associativity\n",
        "\n",
        "Hence the results follows.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChLXn5kq-ccf"
      },
      "source": [
        "**Review of Gaussian elmination**\n",
        "\n",
        "Given the matrix equation $Ax = b$ with unknown vector $x$ that needs to be determined,\n",
        "\n",
        "Gaussian elimination method proceeds by systematically multiplying the equation on the left side by elementary row matrices - which are lower triangular with all diagonals '1' leading to $A$ becoming upper triangular.\n",
        "\n",
        "$E_1 Ax =  E_1b$\n",
        "\n",
        "$E_2 E_1 Ax = E_2 b$\n",
        "\n",
        "...\n",
        "\n",
        "$E_r E_{r-1} ... E_1 A x = E_r E_{r-1} ... E_1 b$\n",
        "\n",
        "\n",
        "The $E_i$ are chosen from the matrix in the previous step (we will call it $A_{i-1}$ by taking $A_{i-1}[ii]$ as the pivot element, and dividing the ith row by it and scaling and substracting from all rows below it to give 0s in the column below this element. In case this element is itself 0, we can look along the row for the first non-zero element (or the largest non-zero element), swap the two columns and proceed. Note that swapping  columns is equivalent to post-multiplication A by the appropriate permutation matrix.\n",
        "\n",
        "We have seen that the product of lower triangular matrices remains lower triangular and hence $E = E_r E_{r-1} ... E_1 $ is a lower triangular matrix.\n",
        "\n",
        "$EAx = Ux = b$ where $U$ is a upper triangular matrix. We can solve the latter by back substitution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI9bwvdo6nob"
      },
      "source": [
        "---\n",
        "\n",
        "**Inverse of a square matrix**\n",
        "\n",
        "Let $A$ be a square $(nxn)$ matrix. Then if there exists a matrix represented as $A^{-1}$ such that\n",
        "\n",
        "$A^{-1} A = A A^{-1} = I $, the identity matrix, then $A^{-1}$ is the inverse of $A$\n",
        "\n",
        "A is than called a *Non-Singular* matrix. Every full-rank square matrix is non-singular.\n",
        "\n",
        "\n",
        "Note that if $B$ is the left inverse of $A$, i.e. $BA = I$  and $C$ is the right inverse of $A$, i.e $AC = I$ then $B=C=A^{-1}$\n",
        "\n",
        "$BI = B => B(AC) = B => (BA)C = B => IC = C = B$\n",
        "\n",
        "We can see that:\n",
        "\n",
        "$(AB)^{-1} = B^{-1}A^{-1}$\n",
        "\n",
        "Similarly: $(ABC)^{-1} = C^{-1}B^{-1}A^{-1}$ and so on.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt-ZZiiF8F-h"
      },
      "source": [
        "---\n",
        "**Finding inverse via Gaussian Elimination**\n",
        "\n",
        "We need to find $B$ such that $AB = I$, i.e. $A\\begin{bmatrix} b_{:1} & b_{:2} & ... & b_{:n} \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0& ... & 0 \\\\ 0 & 1 & 0 & ... & 0 \\\\ ... \\\\ 0 & 0 & 0 & ... & 1 \\end{bmatrix}$\n",
        "\n",
        "So we need to solve $n simultaneous equations:\n",
        "\n",
        "$A b_{:1} = e_1$\n",
        "\n",
        "$A b_{:2} = e_2$\n",
        "\n",
        "...\n",
        "\n",
        "$Ab_{:n} = e_n$\n",
        "\n",
        "By the Gaussian elimination procedure, we can find a lower triangular matrix $E$ such that $EA = U$ an upper triangular matrix.\n",
        "\n",
        "So we can do $EAB = UB = EI$ and we can solve for each unknown column of $B$ by back substitution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1CO0aMT0moW"
      },
      "source": [
        "---\n",
        "**Inverse of a lower triangular matrix with diagonal 1s**\n",
        "\n",
        "Let $L = \\begin{bmatrix} 1 & 0 & 0 & ... & 0 \\\\ l_{21} & 1 & 0 & ... & 0 \\\\ l_{31} & l_{32} & 1 & ... & 0 \\\\ ... \\\\ l_{n1} & l_{n2} & l_{n3} & ... & 1 \\end{bmatrix}$ be a lower triangular matrix with non-zero diagonal entries\n",
        "\n",
        "\n",
        "Find an $E$ such that $EL = I$\n",
        "\n",
        "If we apply Gaussian elimination process, we get\n",
        "\n",
        "$E_1  = \\begin{bmatrix} 1 & 0 & 0 & ... & 0 \\\\ -l_{21} & 1 & 0 & ... & 0 \\\\ -l_{31} & 0 & 1 &  ... & 0 \\\\ ... \\\\ -l_{n1} & 0 & 0 & ... & 1 \\end{bmatrix}$\n",
        "\n",
        "and so on to give $EL = U=I$ on left side and $EI = E$ on right side. Therefore $E = L^{-1} = \\begin{bmatrix} 1 & 0 & .. & 0 \\\\ -l_{21} & 1 & .. & 0 \\\\ ... \\\\ -l_{n1} & -l_{n2} & .. & 1 \\end{bmatrix} $\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj-FBqWYDyRC"
      },
      "source": [
        "---\n",
        "**LU Factorization**\n",
        "\n",
        "Therefore.  $EAx = Ux = Eb = L^{-1}b$\n",
        "\n",
        "In other words. $EA = L^{-1}A = U => A = LU$\n",
        "\n",
        "Thus we have factorized a square matrix $A$ via Gaussian Elimination into a product of lower triangular and upper triangular matrix.\n",
        "\n",
        "While $L$ has 1s in the diagonal, the upper triangular matrix $U$ might not have. We can pull out the diagonals and write\n",
        "\n",
        "$A = LDU$ where L, U are lower and upper triangular with 1s in the diagonal and D is a diagonal matrix.\n",
        "\n",
        "Note that we can factorize $D=D_1 D_2$ in arbitary ways and hence $A= (LD_1)(D_2U)$ leads to infinitely many LU factorizations $LD_1$ is lower triangular and $D_2U$ is upper triangular.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZqfVHPQqtOJ"
      },
      "source": [
        "---\n",
        "However if $A=LDU$ and $L$ and $U$ are lower and upper triangular respectively with  ones in the diagonal, and if $A$ is invertible, the factorization is unique.\n",
        "\n",
        "If $A$ is invertible, so are $L, D, U$.\n",
        "\n",
        "Let $A = L_1 D_1 U_1 = L_2 D_2 U_2$ be two possible factorizations.\n",
        "\n",
        "Then $D_1U_1U_2^{-1} = L_1^{-1}L_2D_2$.\n",
        "\n",
        "Note that $U_2^{-1}$ is upper triangular and $L_1^{-1}$ is lower triangular. So in the above equation, lower triangular is equal to upper triangular which can only happen if off-diagonal entries are 0 and diagonal entries are same i.e. $D_1 = D_2$\n",
        "\n",
        "This also means that $L_1^{-1}L_2 = I$ which means $L_1 = L_2$. Similarly $U_1 = U_2$\n",
        "\n",
        "Thus if $A$ invertible, there is a unique $LDU$ factorization with $L,U$ lower and upper triangular with diagonals 1.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDUY_geqG9CC"
      },
      "source": [
        "---\n",
        "**Number of multiplications for find LU factorization**\n",
        "\n",
        "$O(n^3)$\n",
        "\n",
        "If we want to use same matrix $A$ repeatedly to solve for different $b$ in $Ax=b$, then we can factorize $A=LU$ and then do\n",
        "\n",
        "$Ly = b$ as forward calculation and $Ux = y$ via back substitition to get $x$.\n",
        "\n",
        "This has $O(n^2)$ multiplications\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5P2lrDuHctG"
      },
      "source": [
        "**Row Echelon Form & Pivots **\n",
        "\n",
        "The structure of $U$ after gaussian elimination tells us which are the linearly independent columns. It has a echelon form (step form) as the boundary between non-zero and zero entries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXZyNtHfiV_4"
      },
      "source": [
        "---\n",
        "**A Brief on Computing Architecture**\n",
        "\n",
        "The computing machine has some arithmetic and logic units (ALU) which are connected to memories. The most significant component of the ALU is the Multiply Accumulate Unit (MAC) which is does a multiply and addition. This is important for matrix-vector multiply (or dot product) :\n",
        "\n",
        "$ s = \\sum_{i=1}^n a_i b_i$ is realized as\n",
        "\n",
        "```python\n",
        "s=0\n",
        "for i in range(n):\n",
        "   s = s + a[i] b[i] #Multiply accumulate\n",
        "```\n",
        "GPU and CPU optimize hardware to speedup this operation - and can be taken advantage of by using vectorized machine instructions. This low level implementation detail is usually hidden in libraries and is exposed as high level API (application programming interfaces).\n",
        "\n",
        "Usually the numbers are floating point (to approximate better the real numbers) and hence the speed is measured in Floating Point Operations per Second (FLOPS)\n",
        "For example, the V100 GPU from Nvidia can do 15.7 Tera FLOPS of single precision arithmetic.\n",
        "\n",
        "Memory is the other important piece here. The data needs to be stored here and it needs to be streamed out to be fed into the MAC units (or execution cores). Memory within the same integrated circuit of GPU or CPU, allows for rapid streaming out and in of operands and results respectively. There is memory outside the CPU/GPU cores - in main memory (DRAMs), which are slower to slurp in from."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1T9jGeBbMeD"
      },
      "source": [
        "---\n",
        "**numpy**\n",
        "\n",
        "is a numerical matrix computation  library in *python*\n",
        "\n",
        "There are many such libraries in other programming languages.\n",
        "\n",
        "We will use the popular numerical linear algebra package numpy\n",
        "https://numpy.org/doc/stable/user/index.html\n",
        "The above website has documentation, tutorials etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJZykwE8aNrm"
      },
      "outputs": [],
      "source": [
        "#import the package for numerical linear algebra computations in Python\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qq29Rdt7dzzA"
      },
      "source": [
        "---\n",
        "**Execution speed comparison of vectorised and non-vectorised code**\n",
        "We will generate a random vector with N entries and then use the timeit function to measure its execution time for two different implementations. A naive one using for loops and a vectorized one.\n",
        "\n",
        "*%%timeit -n num_loops -r num_runs*\n",
        "at the top of a cell, instruments it to measure execution time of the cell.\n",
        "\n",
        "The content of a cell are run back-to-back *num_loops* times each run and the execution time is measured. Thus the execution time of the code in the cell, measured for a single run:  $\\frac{execution\\_time}{num\\_loops}$.\n",
        "\n",
        "This repeated *num_run* times to collect execution times across different runs and then averaged and std_dev is obtained.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufR99eBfZz6-"
      },
      "outputs": [],
      "source": [
        "\n",
        "N=10000\n",
        "x = np.random.rand(N,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9WDtELuajSf",
        "outputId": "0f077dca-01c8-4cbb-bb9d-4954a0325cec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.94 ms ± 490 µs per loop (mean ± std. dev. of 30 runs, 100 loops each)\n"
          ]
        }
      ],
      "source": [
        "#find average\n",
        "%%timeit -n 100 -r 30\n",
        "sum = 0\n",
        "for i in range(len(x)):\n",
        "  sum = sum + x[i,0]\n",
        "\n",
        "avg = sum/N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJTDV2EbcIxK",
        "outputId": "43bcb70f-a7df-4cd1-971d-f1cd99572b85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11.5 µs ± 4.29 µs per loop (mean ± std. dev. of 30 runs, 100 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 100 -r 30\n",
        "avg = np.ones(N) @ x / N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZw2ywA5g1Ks",
        "outputId": "cdef9dc7-142d-4b53-e8c0-e7cbc90e117b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9.31 µs ± 2.73 µs per loop (mean ± std. dev. of 30 runs, 100 loops each)\n"
          ]
        }
      ],
      "source": [
        "#alternative using np.sum\n",
        "%%timeit -n 100 -r 30\n",
        "avg = np.sum(x)/N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Yt3wtC_e816",
        "outputId": "ac0dd91d-a95a-4543-cd24-7db4e38a0885"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25.3 ms ± 5.15 ms per loop (mean ± std. dev. of 30 runs, 100 loops each)\n"
          ]
        }
      ],
      "source": [
        "#find std deviation\n",
        "%%timeit -n 100 -r 30\n",
        "sum = 0\n",
        "for i in range(len(x)):\n",
        "  sum = sum + (x[i,0]-avg)**2\n",
        "\n",
        "std_dev = np.sqrt(sum/N)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvLlAAzifcvH"
      },
      "source": [
        "---\n",
        "**Q2: Vectorized implementation for calculating std dev**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81PvauCefbjL",
        "outputId": "7fbca8fa-6007-48e6-d3e6-2d7d6db52116"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21.1 µs ± 6.29 µs per loop (mean ± std. dev. of 30 runs, 100 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 100 -r 30\n",
        "avg = np.sum(x)/N\n",
        "#std_dev = <your code here>\n",
        "std_dev = np.sqrt(((x-avg).T @ (x-avg))/N)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSmKG9U0vC4s"
      },
      "source": [
        "---\n",
        "**Q3**\n",
        "\n",
        "Find the a) rank of A and b) the basis vectors of its null space, given that:\n",
        "\n",
        "$A = L U = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 3 & 1 & 0 & 0 \\\\ 4 & 1 & 3 & 0 \\\\ 6 & 4 & 3 & 2 \\end{bmatrix} \\begin{bmatrix} 1 & 3 & 0 & 1 & 2 & 1 \\\\ 0 & 0 & 3 & 4 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 3\\\\ 0 & 0 & 0 & 0 & 0 & 0 \\end{bmatrix}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWX8ibYyv62N"
      },
      "source": [
        "Ans:\n",
        "\n",
        "$rank(A) = rank(L)rank(U)$. $rank(L)=4$ as it is lower triangular with all non-zero diagonal. $rank(U) = 3$ has it has 3 non-zero pivots. The linearly independant columns of $U$ are $1, 3, 6$.\n",
        "\n",
        "By taking a sub-matrix of columns $U^{'}=\\begin{bmatrix}U_{:1}& U_{:3} & U_{:6} \\end{bmatrix}$ and then finding the solutions $v_1, v_2 \\& v_3$ such that $U^{'}v_1=U_{:2}$,  $U^{'}v_2=U_{:4}$ $U^{'}v_3=U_{:5}$\n",
        "\n",
        "allows us to construct the basis vectors\n",
        "\n",
        "$b_1=\\begin{bmatrix}v_1[1] & -1 & v_1[2] & 0 & 0 & v_1[6]\\end{bmatrix}^T$\n",
        "\n",
        "$b_2=\\begin{bmatrix}v_2[1] & 0 & v_2[2] & -1 & 0 & v_2[6]\\end{bmatrix}^T$\n",
        "\n",
        "$b_3=\\begin{bmatrix}v_3[1] & 0 & v_3[2] & 0 & -1 & v_3[6]\\end{bmatrix}^T$\n",
        "\n",
        "Hence one basis of the null space is $\\{ \\begin{bmatrix} 3 & -1 & 0 & 0 & 0 & 0\\end{bmatrix}^T, \\begin{bmatrix} 1 & 0 & 4/3 & -1 & 0 & 0 \\end{bmatrix}^T,\\begin{bmatrix} 2 & 0 & 0 & 0 & -1 & 0 \\end{bmatrix}^T\\}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJcL0vomhOln"
      },
      "source": [
        "---\n",
        "**Q4**\n",
        "Find all 2x2 matrices $A$ such that $A^2=I$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChJYeMaiOR7e"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXW1SDw7jVv4"
      },
      "source": [
        "Ans:\n",
        "\n",
        "Let $A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}$\n",
        "\n",
        "Then $A^2 = \\begin{bmatrix} a^2+bc & ab+bd \\\\ ac + cd & bc + d^2 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$\n",
        "\n",
        "Equating the entries, we have the following:\n",
        "\n",
        "I: If both b=c=0, then $A=I$ or $A=-I$\n",
        "\n",
        "\n",
        "III: if one of b,c $\\neq$ 0, the $a = \\pm \\sqrt{1 - bc} = -d$. Note for $bc < 1$, a, dwill be imaginary and a= \\$i\\sqrt{bc-1}$ = -d\n",
        "\n",
        "Therefore $A = \\begin{bmatrix} \\pm\\sqrt{1-bc} & b \\\\ c & \\mp\\sqrt{1-bc} \\end{bmatrix}$.   \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-ySUrR6PAWv"
      },
      "source": [
        "---\n",
        "**Symmetric and Skew-Symmetric Matrices**\n",
        "\n",
        "$A$ is a symmetric matrix if $A^T = A$\n",
        "\n",
        "if $A$ is symmetric and invertible, then  $A = LDU$ then $A^T = U^TDL^T = LDU$ and $L=U^T$ by uniques of factorization (see above). Therefore for symmetric matrices $A = LDL^T$\n",
        "\n",
        "$A$ is skew symmetric if $A^T = -A$\n",
        "Note that the diagonal values of a skew symmetric matrix are all zeros.\n",
        "\n",
        "Example: $\\begin{bmatrix} 0 & 3 & 2 \\\\ -3 & 0 & 1 \\\\ -2 & -1 & 0 \\end{bmatrix}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVBDjuxFn-qs"
      },
      "source": [
        "---\n",
        "**Vector product**\n",
        "\n",
        "In 3D, we are used to vectors represented as $\\overrightarrow{v} = v_1 \\overrightarrow{i} + v_2 \\overrightarrow{j} + v_3 \\overrightarrow{k}$ and $\\overrightarrow{w} = w_1 \\overrightarrow{i} + w_2\\overrightarrow{j} + w_3 \\overrightarrow{k}$\n",
        "\n",
        "We have the notion of cross product of two vectors, which gives a third vector which is normal to both. Note that any  two vectors define a plane and the cross product is a vector normal to the plane.\n",
        "\n",
        "The cross product is defined as $\\overrightarrow{w} \\times \\overrightarrow{v} = (w_2v_3 - w_3v_2)\\overrightarrow{i} + (w_3v_1 - w_1v_3) \\overrightarrow{j} + (w_1v_2 - w_2v_1) \\overrightarrow{k}$\n",
        "\n",
        "\n",
        "We can calculate vector product through matrix vector multiplication, if we represent $\\overrightarrow{w}$ as a skew symmetric matrix as: $[w]_\\times = \\begin{bmatrix} 0 & -w_3 & w_2 \\\\ w_3 & 0 & -w_1 \\\\ -w_2 & w_1 & 0 \\end{bmatrix}$\n",
        "\n",
        "Now $\\omega \\times v = [w]_\\times v = \\begin{bmatrix} 0 & -w_3 & w_2 \\\\ w_3 & 0 & -w_1 \\\\ -w_2 & w_1 & 0 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{bmatrix}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-c96MjURWhW"
      },
      "source": [
        "---\n",
        "**Q6 Vector product**\n",
        "\n",
        "a) What is $v \\cdot \\omega \\times v$\n",
        "\n",
        "b) What is $\\omega \\times \\omega$\n",
        "\n",
        "\n",
        "c) What is the rank of $[\\omega]_\\times$ (*Hint*: You can guess from its operation of realizing the cross product)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp2jOZVzeb5c"
      },
      "source": [
        "Ans:\n",
        "\n",
        "a) $v \\cdotp \\omega \\times v = v^T [\\omega]_\\times v = 0$\n",
        "\n",
        "as $\\omega \\times v$ give a vector perpedicular to the plane formed by $v$ and $\\omega$\n",
        "\n",
        "b) $\\omega \\times \\omega = 0 $\n",
        "\n",
        "c) Since $[\\omega]_\\times$ maps a vector $v$ to its cross product with $\\omega$, any vector collinear with $\\omega$ will map to $0$. Hence the $dim(NullSpace([\\omega]_\\times)) = 1$ and $rank([\\omega]_\\times) = 2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLot_90rpnME"
      },
      "source": [
        "---\n",
        "**Matrix Product and Matrix Group**\n",
        "\n",
        "Associativity: $A(BC) = (AB)C$ provided the multiplication is possible in terms of the matrix dimensions.\n",
        "\n",
        "Not commutative: $AB \\ne BA$ in general (even when A & B are square)\n",
        "\n",
        "Consider the set of $nxn$ non-singular matrices $GL(n) = \\{A \\in ℜ^{nxn}\\}$ along with the matrix product '·'. This is a \"Group\" and called the General linear group:\n",
        "\n",
        "1. Closure under multipication: $A, B \\in GL(n)$ then $A·B \\in GL(n)$\n",
        "2. Associative: $A, B, C \\in GL(n)$ then $(A·B)·C = A·(B · C)$\n",
        "3. Identity: $I_n \\in GL(n)$ such that $A · I_n = I_n · A = A$\n",
        "4. Inverse: $∀\\ A \\in GL(n)$ then $∃\\ A^{-1}$ such that $A · A^{-1} =I_n = A^{-1}A$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCpRXl6ptiVp"
      },
      "source": [
        "---\n",
        "**Q7**\n",
        "\n",
        "Which of these matrix sets are groups under usual matrix multiplication and Inverse?\n",
        "\n",
        "a) Set of symmetric matrices\n",
        "\n",
        "b) Set of Positive Matrices (all entries are strictly greater than 0)\n",
        "\n",
        "c) Set of diagonal invertible matrices\n",
        "\n",
        "d) Set of permutation matrices (Permutation matrix has exactly single 1 in each row and column and zeros every where else)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a987ThzOu_0Q"
      },
      "source": [
        "Ans:\n",
        "\n",
        "a) No. $A, B$ is symmetric, then $(AB)^T = BA \\neq AB$ in general (you can construct examples for 2x2 symmetric matrices to check this) and is not within the set. Hence not closed and therefore not a group.\n",
        "\n",
        "b) No. $I$ does not belong to this set\n",
        "\n",
        "c) Yes. An invertible diagonal matrix has non-zero on diagonal and zero everywhere.  $I$ belongs to this set. Product of two invertible diagonal matrix is a diagonal matrix which is invertible\n",
        "\n",
        "d) Yes. $I$ belongs to this set. Product of permutation matrix is also a permutation matrix hence closed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn8gZe-K8F0D"
      },
      "source": [
        "---\n",
        "**A Brief on Number Representations**\n",
        "\n",
        "Floating point numbers are presented as below (https://en.wikipedia.org/wiki/Floating-point_arithmetic):\n",
        "\n",
        "Type | Total Bits | sign | exponent | mantissa | bits of precision | exponent bias  | decimal digits(mantissa)\n",
        "-----|------------|------|----------|----------|-------------------|----------|------------\n",
        "Half Precision | 16 | 1 | 5 | 10 | 11 | 15 | ~3.3\n",
        "Single Precision|32 | 1 | 8 | 23 | 24 |127 | ~7.2\n",
        "Double Precision|64 | 1 | 11| 52 | 53 |1023|~15.9\n",
        "Extended Precision (x86)|80| 1| 15| 64 | 64 |16383|~19.2\n",
        "\n",
        "Lets take 16-bit numbers as an example.\n",
        "the left most bit: is the sign bit.\n",
        "Next 10 bits are the mantissa (or significand).\n",
        "Last 5 bits are exponent.\n",
        "\n",
        "Normalized numbers have a binarypoint after the first bit and the first bit is always 1. For example, a binary number 0.001x$2^0$ is represented as 1.000 x $2^{-3}$. That is, you left shift till the first non-zero bit comes to the first location (and corresponding adjust the exponent).\n",
        "\n",
        "Since the first bit is '1' for a normalized representation, we dont need to store it. Hence we effectively have 10+1 = 11 bits of precision. Precision refers to the accuracy of representation. The gap between two numbers representatble by the mantissa is $2^{-10}$ and the round-off error is half of that and is $2^{-11}$\n",
        "\n",
        "The exponent takes values from 00000 (0) to 11111 (31). All 1's is used to represent infitinity and NaN and all 0 for sub-normal numbers (where the first bit is not 1) Hence the exponent ranges from 1 to 30. With exponent bias of 15, the represented exponent is subtracted by the bias  to give the actual exponent. for example, an exponent if -11 is represented as -11+15 = 4. Hence the representable exponents range from -14 to 15 (30 values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaK-NJ9K6Fln",
        "outputId": "26504723-9a5d-4148-96c8-e984d0e11022"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "float16 info: Machine parameters for float16\n",
            "---------------------------------------------------------------\n",
            "precision =   3   resolution = 1.00040e-03\n",
            "machep =    -10   eps =        9.76562e-04\n",
            "negep =     -11   epsneg =     4.88281e-04\n",
            "minexp =    -14   tiny =       6.10352e-05\n",
            "maxexp =     16   max =        6.55040e+04\n",
            "nexp =        5   min =        -max\n",
            "smallest_normal = 6.10352e-05   smallest_subnormal = 5.96046e-08\n",
            "---------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"float16 info:\",np.finfo(np.float16) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWzrzcSu8v6A"
      },
      "source": [
        "**Round off errors**\n",
        "\n",
        "Since the floating point representation can only represent finite numbers of values on the real line, there will be round off error of about $2^{-p}/2$.\n",
        "\n",
        "As an example below, the real number 0.1 can only be represented within $0.02\\%$ with 16-bit floating point numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmbHGo3O_r4d",
        "outputId": "1f325d05-173f-4432-ae7b-de2be5ffb580"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Percentage error in value stored in f16=-0.02441406250000555%, and occupies 2 bytes\n",
            "Error in value stored in f32=1.4901161138336505e-06%, and occupies 4 bytes\n",
            "Error in value stored in f64=0.0%, and occupies 8 bytes\n"
          ]
        }
      ],
      "source": [
        "#declare a variable to be half precision floating point type and print its characteristics\n",
        "f16 = np.float16(\"0.1\")\n",
        "f32 = np.float32(\"0.1\")\n",
        "f64 = np.float64(\"0.1\")\n",
        "print(f\"Percentage error in value stored in f16={(f16-0.1)*100/0.1}%, and occupies {f16.itemsize} bytes\")\n",
        "print(f\"Error in value stored in f32={(f32-0.1)*100/0.1}%, and occupies {f32.itemsize} bytes\")\n",
        "print(f\"Error in value stored in f64={(f64-0.1)*100/0.1}%, and occupies {f64.itemsize} bytes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTuRFIUO9z3h"
      },
      "source": [
        "---\n",
        "**Well Conditioned and Ill-Conditioned**\n",
        "\n",
        "Ref: https://en.wikipedia.org/wiki/Condition_number\n",
        "\n",
        "Regardless of the round off error, the nature of the matrix (or in general a function) will be such that it is very sensitive to inputs.\n",
        "\n",
        "For example, in solving $Ax=b$, the input is $b$ and output $x$. Depending on the condition number of the matrix, a small perturbation of $b$ can lead to a large change in the solution $x$.\n",
        "\n",
        "The example below illustrates this. We perturb $b$ to $d$ and $e$ and look at the changes in the soliutions.\n",
        "\n",
        "The ratio of the relative change in output  (i.e. change in solution) to relative change in input  (change in b) ) is  equal to the condition number (kind of like an amplifier gain).\n",
        "\n",
        "\n",
        "Too high a condition number means a ill-conditioned system. Any small errors in the input will through the solution off.\n",
        "\n",
        "The condition number of a matrix is relted the ratio of its largest singular value to the smallest singular value (which in turn is related to the eigen values for a square matrix)\n",
        "\n",
        "**Singular Value Decomposition**\n",
        "\n",
        "Any matrix $A \\in ℜ^{m \\times n}$ can be decomposed as:\n",
        "\n",
        "$A = U \\Sigma V^T$\n",
        "\n",
        "Where $U \\in ℜ^{m \\times m}$ and $V in ℜ^{n \\times n}$ are orthogonal matrices, i.e.,  $U^TU=I$ and $V^TV=I$ and $\\Sigma = [\\sigma_{ij}]$ is a $m \\times n$ diagonal matrix with non-negative values in the main diagonal ($\\sigma_{ii} \\geq 0$ and \\sigma_{ij} = 0 $ for $i \\neq j$ )\n",
        "\n",
        "$\\sigma_{ii}$ written simply as $\\sigma_i$ are called the singular values.\n",
        "\n",
        "For a matrix of rank $r$, there are exactly $r$ non-zero singular values.\n",
        "\n",
        "$AV = U\\Sigma$\n",
        "\n",
        "$U^TA = \\Sigma V$\n",
        "\n",
        "The columns of $V$ and $U$ form the right-singular and left-singular verctors respectively.\n",
        "\n",
        "If the singular values are usually arranged in a non-increasing order, i.e. $\\Sigma=Diag(\\sigma_1, \\sigma_2, ..., \\sigma_r,0,..)$ where $r$ is the rank of the matrix\n",
        "\n",
        "Condition number of the matrix $κ(A) = \\frac{\\sigma_{max}}{\\sigma_{min}}$\n",
        "\n",
        "Note, the right nullspace of A is the set of columns of V with singular value 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBd6367n_NZs"
      },
      "source": [
        "---\n",
        "**Orthogonality**\n",
        "\n",
        "Two vectors are orthogonal if their inner product (or dot product) is 0.\n",
        "\n",
        "If $v_1, v_2, ... , v_k$ are pairwise orthogonal then they are linearly independent.\n",
        "\n",
        "To see this, let $c_1 v_1 + c_2 v_2 + .... c_k v_k = 0$, then taking the dot product with $v_j$ on both sides gives $c_j v_j \\cdotp v_j = 0$ and hence $c_j=0$ for all j.\n",
        "\n",
        "\n",
        "*Orthogonality of Row space with NullSpace*\n",
        "\n",
        "Let $A: V_n → W_m$ . Then\n",
        "\n",
        "Null Space $N(A) = \\{v \\in V_n: Av = 0\\} $\n",
        "\n",
        "Row Space $R(A) = \\{v \\in V_n: s.t.\\ \\ v^T = w^T A, w \\in W_m\\}$\n",
        "\n",
        "Then if $x \\in R(A)$ then $x^T = u^TA$ for some $u \\in W_m$\n",
        "Let $y  \\in N(A)$ then\n",
        "\n",
        "$x^Ty = (u^TA)y = u^T(Ay) = 0$\n",
        "\n",
        "Thus $R(A) \\perp N(A)$ and each is called the orthogonal complement of the other.\n",
        "\n",
        "\n",
        "Example:\n",
        "\n",
        "$A = \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix}$ has rank 1, which is the rank of the $RowSpace(A)$.\n",
        "\n",
        "$N(A) = \\{\\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}\\}$ such that $\\begin{bmatrix} x & y & z \\end{bmatrix}\\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix} = ax + by +cz = 0\\}$ is the orthogonal 2D plane."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3nV0-stvntG",
        "outputId": "4e5adbea-e070-4217-fe30-59eb0e75616d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x=[2. 0.], y=[-7.9976163  9.997616 ], cond(A)=39995.3671875\n",
            "||b-d|| = 0.0009999275207519531, ||x-y||=14.138764381408691\n",
            "||b-e|| = 0.009000062942504883, ||x-z||=127.25898742675781\n"
          ]
        }
      ],
      "source": [
        "A = np.array([[1,1],[1,1.0001]],np.float32)\n",
        "b = np.array([2,2],np.float32)\n",
        "d = np.array([2.0,2.001],np.float32)\n",
        "e = np.array([2.0,2.009],np.float32)\n",
        "\n",
        "x = np.linalg.solve(A,b)\n",
        "y = np.linalg.solve(A,d)\n",
        "z = np.linalg.solve(A,e)\n",
        "\n",
        "print(f\"x={x}, y={y}, cond(A)={np.linalg.cond(A,p=2)}\")\n",
        "print(f\"||b-d|| = {np.linalg.norm(b-d)}, ||x-y||={np.linalg.norm(x-y) }\")\n",
        "print(f\"||b-e|| = {np.linalg.norm(b-e)}, ||x-z||={np.linalg.norm(x-z) }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3qgP2tAr7xm"
      },
      "source": [
        "---\n",
        "We know that column space of $A$ is $C(A) = \\{w = Av\\ \\forall\\ v \\in V_n\\}$\n",
        "\n",
        "$dim(C(A)) = dim(R(A)) = rank(A)$\n",
        "\n",
        "We can define left null space of $A$ as $LN(A) = \\{w | w^TA = 0\\}$\n",
        "\n",
        "Note that $LN(A) \\perp C(A)$\n",
        "\n",
        "This leads to the nice diagram of the front cover of the Strang textbook.\n",
        "\n",
        "Note that we can do a one-to-one map of $R(A)$ with $C(A)$ as:\n",
        "\n",
        "$A:R(A) → C(A)$ with $x_r \\in R(A)$ and $Ax_r = b \\in C(A)$\n",
        "\n",
        "This is a one-to-one mapping as otherwise if $Ax_r^{'} = b$, then $A(x_r^{'}-x_r) = 0$ and $x_r^{'}-x_r \\in N(A)$.\n",
        "\n",
        "However since $R(A)$ is a vector subspace, $x_r^{'}-x_r \\in R(A)$ which can only happen if there difference is 0 and hence they are equal.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUTzrI--r0oV"
      },
      "source": [
        "---\n",
        "**Least Squares Solution**\n",
        "Let $A:V_n → W_m$ and $rank(A) = k \\leq min(m,n)$\n",
        "\n",
        "Then $A^TA$ is a $nxn$ matrix and $AA^T$is a $mxm$ matrix, each with rank $k$.\n",
        "\n",
        "Both are symmetric.\n",
        "\n",
        "Lets assume that $m > n$ and $rank(A) = n$, that is more rows than columns.\n",
        "\n",
        "Then $Ax =b $ need not have any solution as $b$ can be outside the the columns space of A (which has a dimension only $n$.\n",
        "\n",
        "This is a common problem in many data fitting problems.\n",
        "\n",
        "*Example*\n",
        "\n",
        "Imagine that we have $m$ observations $\\{(a_i,b_i\\}$ and we want to fit a polynomial equation of degree $n-1$ to fit this. i.e\n",
        "\n",
        "$y = \\sum_{k=0}^{n-1}z_k x^k$\n",
        "\n",
        "where we want to find the best estimates of the unknown parameters $\\{z_k\\}$\n",
        "\n",
        "using the observed data, we can write $m$ simultaneous equations\n",
        "\n",
        "$\\sum_{k=0}^{n-1}z_k a_i^k = b_i$ one per observation $(a_i, b_i)$\n",
        "\n",
        "this can be rewritten as a matrix equaiton:\n",
        "\n",
        "$Az = b$ where $A$ is a $mxn$ matrix and\n",
        "\n",
        "$A = \\begin{bmatrix} 1 & a_1 & a_1^2 & ...& a_1^{n-1} \\\\ 1 & a_2 & a_2^2 & ... & a_2^{n-1} \\\\ ... \\\\ 1 & a_m & a_m^2 & ... & a_m^{n-1}\\end{bmatrix}$ and $b=\\begin{bmatrix} b_1 \\\\ b_2 \\\\ ... \\\\ b_m \\end{bmatrix}$\n",
        "\n",
        "\n",
        "**NOTE** the equations are linear in the unknowns and can be non-linear in the data terms.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKWcdLvM1riB"
      },
      "source": [
        "---\n",
        "**Geometric Solution to Least Squares**\n",
        "\n",
        "We want to find $x^{'}$ such that error $e=b-Ax^{'}$ is minimum.\n",
        "\n",
        "From geometry, we know that this happens when error is perpendicular to column space, i.e. $e \\perp C(A)$. Hence $e \\in LN(A)$ is in left nullspace of $A$.\n",
        "\n",
        "Hence $A^Te = A^T(b-Ax^{'}) = 0$ which can be written as\n",
        "\n",
        "$A^TAx^{'} = A^Tb$, also called as the **Normal** Equation\n",
        "\n",
        "\n",
        "and hence $x^{'} = (A^TA)^{-1}A^Tb$ is the solution such that $Ax^{'} = A(A^TA)^{-1}A^Tb$ is closest to b.\n",
        "\n",
        "Since we asumed rank(A) = n, rank($A^TA$) = n and it is invertible.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnjlXKguspYy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89dU4hRm6SaN"
      },
      "source": [
        "---\n",
        "**Projection Matrix**\n",
        "\n",
        "The matrix $P = A(A^TA)^{-1}A^T$ is called the projection matrix and it projects $b$ to $C(A)$\n",
        "\n",
        "Note that $P^2 = P$ (check).\n",
        "\n",
        "rank(P) = n\n",
        "\n",
        "\n",
        "In general, if we are given $\\{v_1, .... v_k\\}$ and want to project another vector $w$ onto the Span($\\{v_1, .... v_k\\}$), we can form a matrix\n",
        "$A = [v_1 v_2 ... v_k]$ and find the projection matrix $P=A(A^TA)^{-1}A^T$ and do $w_A=Pw$ to do the projection.\n",
        "\n",
        "Hence, in general if $S \\subset V$ a subspace and $P_S$ be the projection matrix that projects any vector onto the subspace, then\n",
        "\n",
        "for any $v \\in V$ we can write\n",
        "\n",
        "$v_S = P_S v$ and $v_\\perp = v - P_S v = (I - P_S)v$ with $v = v_S + v_\\perp$\n",
        "\n",
        "Therefore a reflection of a vector $v$ about a subspace can be written as:\n",
        "\n",
        "$v_r = v_\\perp - v_S = (I-2P_S)v$\n",
        "\n",
        "Hence $H=I-2P_S$ is the reflection matrix about a subspace $S$. This is like a ray of light reflecting of the surface\n",
        "\n",
        "Note -$-v_r$ is also another kind of reflection (mirror reflection)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVHPD5Nv7Skl"
      },
      "source": [
        "---\n",
        "**Optimization based derivation of least means square solution**\n",
        "\n",
        "The residual error is $e = b-Ax$ and we would like to minimize its $L_2$ norm as:\n",
        "\n",
        "$min_x ||e||^2$ where\n",
        "\n",
        "$||e||^2 = e^Te  = (b-Ax)^T (b-Ax) = x^TA^TAx -b^TAx - x^TA^Tb + b^Tb$\n",
        "\n",
        "Note that $b^TAx = x^TA^Tb$ as it is a scalar\n",
        "\n",
        "We have $n$ elements in the variable $x$ and we should differentiate with resepect to each and put the derivative to 0.\n",
        "\n",
        "This can be compactly done using vector differentiation or gradient as:\n",
        "\n",
        "$\\nabla L = 2A^TAx-2A^Tb = 0$ (at extrema. we have used L to represent the square error). The extrema point $x^{'} = (A^TA)^{-1}A^Tb$ same as derived via geometric consideration.\n",
        "\n",
        "The equivalent for double derivative is called the Hessian and is $A^TA$ which is positive semi definite in general and positive definite for vectors not in the null space of A (equivalent to second derivative being positive) and hence the extrema is a minima.\n",
        "\n",
        "A matrix $B$ is positive definite is $x^TBx > 0 $ for all $x \\neq 0$.\n",
        "\n",
        "Since $x^T(A^TA)x = (x^TA^T)(Ax) = ||Ax||^2$ it is positive definite. I.e. the shape of the loss function at this point is \"upward facing parabola\" as anydirection you move, will only increase L.\n",
        "\n",
        "\n",
        "The benefit of this optimization approach is that we can add *regularization*, ie additional constraints on the parameters. for example we can look for parameters that are not too large in magnitude. i.e. $||x||^2$ is kept small. Or we might also bring in weights for different error terms via a diagonal weight matrix $e = W(b-Ax)$\n",
        "\n",
        "So $L = e^Te + \\lambda x^Tx$ is the loss function to be minimized and\n",
        "\n",
        "$\\nabla L = (A^TW^TWA+\\lambda I)x^{'}-A^TW^TWb=0$ and hence\n",
        "\n",
        "$x^{'} = (A^TW^TWA+\\lambda I)^{-1}A^TW^TWb$\n",
        "\n",
        "is the optimal solution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU2edhEjLAfu"
      },
      "source": [
        "---\n",
        "**Q7**\n",
        "\n",
        "Find the weighted least squares solution to $A = \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 1 & 2 \\end{bmatrix}$ $b = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix}$ and $W=\\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRG2Vn5IMZFi",
        "outputId": "0f5612ad-e669-43e4-b5e5-86fc40151bb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x=[0.04761905 0.57142857], Ax = [0.04761905 0.61904762 1.19047619],\n",
            " e = [ 0.04761905 -0.38095238  0.19047619] and ||e||=0.6546536707079771\n"
          ]
        }
      ],
      "source": [
        "#Ans\n",
        "\n",
        "A = np.array([[1., 0],[1,1],[1,2]])\n",
        "b = np.array([0,1,1])\n",
        "W = np.array([[2,0,0],[0,1,0],[0,0,1]])\n",
        "\n",
        "#Your Answer Here\n",
        "x = np.linalg.inv(A.T @ W.T @ W @ A) @ A.T @ W.T @ W @ b\n",
        "e = A@x - b\n",
        "print(f\"x={x}, Ax = {A@x},\\n e = {e} and ||e||={np.sqrt(np.linalg.norm(e))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70Gu85QfObnI"
      },
      "source": [
        "---\n",
        "**Q8**\n",
        "If $V = Span(\\{[1,1,0,1]^T, [0,0,1,0]^T\\})$, find\n",
        "\n",
        "a) a basis for orthogonal complement of $V$ which is $V^\\perp$\n",
        "\n",
        "b) The projection matrix $P$ onto V\n",
        "\n",
        "c) The vector in $V$ closest to $b=[0,1,0,-1]$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15Xe-ve3PCl4"
      },
      "source": [
        "Ans:\n",
        "\n",
        "a)\n",
        "\n",
        "Let $A = \\begin{bmatrix} 1 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\end{bmatrix}$\n",
        "\n",
        "$V = C(A)$ column space of A.\n",
        "\n",
        "$V^\\perp$ is the left null space. Since rows 1,2, 4 identical, we can infer the basis vectors of the left null space as $\\{[-1,0,0,1]^T,[0,-1,0,1]^T\\}$\n",
        "\n",
        "b) $P = A(A^TA)^{-1}A^T$\n",
        "\n",
        "c) $Pb$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZXCg-diQ_cO",
        "outputId": "d0dcb2c7-b7df-4507-c1fc-745860c1168b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P=[[0.33333333 0.33333333 0.         0.33333333]\n",
            " [0.33333333 0.33333333 0.         0.33333333]\n",
            " [0.         0.         1.         0.        ]\n",
            " [0.33333333 0.33333333 0.         0.33333333]] and closest vector in V to b =[0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "#Ans contd.\n",
        "A=np.array([[1,0],[1,0],[0,1],[1,0]])\n",
        "b = np.array([0,1,0,-1])\n",
        "\n",
        "#Your code here\n",
        "P = A @ np.linalg.inv(A.T @ A) @ A.T\n",
        "Pb = P @ b\n",
        "\n",
        "print(f\"P={P} and closest vector in V to b ={Pb}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ayxa88H9Sn1F"
      },
      "source": [
        "---\n",
        "**Q9**\n",
        "\n",
        "Let $P$ Project onto Subspace $S$ and $Q$ project onto its orthogonal complement $S^\\perp$.\n",
        "\n",
        "a) What is $P+Q$\n",
        "\n",
        "b) What is $PQ$\n",
        "\n",
        "c) Show that $P-Q$ is its own inverse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uwgy9HveS-DC"
      },
      "source": [
        "Ans:\n",
        "\n",
        "a) let $x$ be any vector then $Px = x_s$ and $Qx=x_{s^\\perp}$ and $x = x_s + x_{s^\\perp}$\n",
        "\n",
        "Therefore $(P+Q)x = Px + Qx = x$ and hence $P+Q=I$\n",
        "\n",
        "b) $PQx = Px_{s^\\perp} = 0$ therefore $PQ=0$. similarly $QP=0$\n",
        "\n",
        "c) $(P-Q)(P-Q) = P^2 -PQ -QP - Q^2 = P + Q = I$\n",
        "since for any projection matrix $P^2=P$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYh5vMaxUZFz"
      },
      "source": [
        "---\n",
        "**Q10**\n",
        "\n",
        "If $P=P^TP$ show that it is a projection matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTt1B70qUf9s"
      },
      "source": [
        "Ans:\n",
        "\n",
        "$P^T = (P^TP)^T = P^TP = P$ hence P is symmetric and $P^2 = P^TP = P$ which means it is a projection matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lK0MHZcU5eo"
      },
      "source": [
        "---\n",
        "**Q11**\n",
        "\n",
        "Let $v_1 = [1,1,0]^T$ and $v_2=[1,1,1]$ be two vectors that span a plane.\n",
        "\n",
        "a) Find the projection matrix $P$ that will project any vector onto that plane\n",
        "\n",
        "b) Find a non-zero vector which projects to 0 when P is applied."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1cSndINVtaV",
        "outputId": "4629c822-91c6-4a30-b312-0baf35824c8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P=[[0.5 0.5 0. ]\n",
            " [0.5 0.5 0. ]\n",
            " [0.  0.  1. ]] b=[ 1 -1  0]\n"
          ]
        }
      ],
      "source": [
        "#Ans\n",
        "\n",
        "A = np.array([[1,1],[1,1],[0,1]])\n",
        "P = A @ np.linalg.inv(A.T @ A ) @ A.T\n",
        "b = np.cross([1,1,0], [1,1,1])\n",
        "print(f\"P={P} b={b}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQogQ4bjKAVf"
      },
      "source": [
        "---\n",
        "**QR Decomposition**\n",
        "\n",
        "Any matrix $A$ which is $m \\times n$ can be decomposed in to a product of a orthogonal matrix $Q$ which is $m \\times m$ w and an upper triangular matrix $R =[r_{ij}]$ which is $m x n$ with $r_{ij}=0$ for $i \\gt j$\n",
        "\n",
        "$A = QR$\n",
        "\n",
        "This is useful for least squares solution as the straightforward solution will need inverse of $A^TA$ which has a condition number of $\\kappa(A)^2$\n",
        "\n",
        "An alternative approach to solve $A^TAx = A^Tb$ is to use $A=QR$, where $Q$ is orthogonal.\n",
        "\n",
        "Hence $||Ax-b||_2 = ||Q^TAx-Q^Tb||_2$ and hence solution of latter gives solution of former.\n",
        "\n",
        "Simplifying further we get the solution of $min_x ||Ax-b|| = min_x ||Rx-Q^Tb||$\n",
        "\n",
        "For the case $m > n$ and full column rank $A$ (i.e rank = n),\n",
        "\n",
        "we can solve the above to obtain: $x_{ls} = R^{-1}Q^Tb$.\n",
        "\n",
        "Note that the condition number of $R$ is same of that $A$ and hence is only $Κ(A)$ and hence offers better numerical stability.\n",
        "\n",
        "\n",
        "*Orthogonal matrix preserves vector lenghts $(L_2)$ and angles*\n",
        "\n",
        "$||Qx||^2 = x^TQ^TQx = x^Tx = ||x||^2$\n",
        "\n",
        "$<Qx,Qy> = x^TQ^TQy = x^Ty = <x,y>$\n",
        "\n",
        "Any matrix which preserves angles and lengths is orthogonal. (You can prove by considering its actions on unit orthogonal vectors)\n",
        "\n",
        "Example:\n",
        "\n",
        "$Q(\\theta) = \\begin{bmatrix} \\cos \\theta & -\\sin \\theta \\\\ \\sin \\theta & \\cos \\theta \\end{bmatrix}$\n",
        "\n",
        "rotates vectors in 2D counter clockwise by $\\theta$\n",
        "\n",
        "$Q^T = Q(-\\theta) = Q^{-1}$ does a rotation clock wise\n",
        "\n",
        "Example:\n",
        "\n",
        "Permutation matrices also preserve lengths and are orthogonal matrices.\n",
        "\n",
        "\n",
        "*Uniqueness of QR factorization**\n",
        "\n",
        "Lets assume that $A$ is non-singular i.e. rank n.\n",
        "\n",
        "Then $A=QR = QDDR$ where $D^2 = I$ the identity matrix. Each element of D is +1 or -1, hence in general $QR$ factorization is not unique. However if we insist on diagonals of $R$ be positive, then factorization is unique.\n",
        "\n",
        "If $rank(A) = k < n$ then factorisation is not unique. Let first $k$ columns of $A$ be linearly independent, then $A = \\begin{bmatrix}q_1 & q_2 &.. &q_k &q_{k+1} &.. &q_n\\end{bmatrix} \\begin{bmatrix} r_{11} &  r_{12} & ..& r_{1k}& r_{1{k+1}}& ..& r_{1n} \\\\ 0 & r_{21} & .. & r_{2k} & r_{2{k+1}} & .. & r_{2n} \\\\ ... \\\\ 0 & 0 & .. & r_{kk} & r_{k{k+1}} & .. & r_{kn} \\\\ 0 & 0 & .. & 0 & 0 & .. & 0 \\\\ ...\\end{bmatrix}$ and $q_{k+1}, ... q_n$ can be any orthogonal basis of null space of $A$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6vqwuHDb258"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu_dtdNcTy4q"
      },
      "source": [
        "---\n",
        "**Q12**\n",
        "\n",
        "If $u$ is a unit vector, then show that $Q=I-2uu^T$ is a orthogonal matrix. (it is infact the reflection about the vector $u$)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCoD1I07cEmi"
      },
      "source": [
        "Ans:\n",
        "\n",
        "$QQ^T = (I-2uu^T)(I-2uu^T) = I^2 +4(uu^T)(uu^T) -4uu^T $\n",
        "\n",
        "The middle term with associativity is $4(uu^T)(uu^T) = 4u(u^Tu)u^T = 4uu^T$\n",
        "\n",
        "hence the result follows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agzX3gTXdQN5"
      },
      "source": [
        "---\n",
        "**Q13**\n",
        "\n",
        "Let $A = \\begin{bmatrix} 1 & 1\\\\2 & -1 \\\\ -2 & 4\\end{bmatrix}$ and $b=[1, 2, 7]^T$\n",
        "\n",
        "\n",
        "a) find orthonormal $q_1, q_2, q_3$ such that the first two span the column space of $A$\n",
        "\n",
        "b) Which fundamental subspace, defined by $A$, does $q_3$ belong to?\n",
        "\n",
        "c) Use QR decomposition to solve $Ax=b$ with least squares solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnwYlAQLerh1"
      },
      "source": [
        "Ans:\n",
        "\n",
        "a) $q_1, q_2$ obtained by code below. $q_3$ is orthogonal to them and so $q_3 = q_1 \\times q_2$ (cross product\n",
        "\n",
        "b) $q_3$ belongs to left nullspace of $A$\n",
        "\n",
        "c) done in code below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msUucrnweIvn",
        "outputId": "b4bf7c8d-354c-42b1-c23a-0bc193f741ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q=[[-0.33333333 -0.66666667]\n",
            " [-0.66666667 -0.33333333]\n",
            " [ 0.66666667 -0.66666667]],\n",
            " R=[[-3.  3.]\n",
            " [ 0. -3.]]\n",
            "Solution x=[1. 2.]\n"
          ]
        }
      ],
      "source": [
        "#Ans:\n",
        "\n",
        "A = np.array([[1,1],[2,-1],[-2,4]])\n",
        "\n",
        "Q,R = np.linalg.qr(A)\n",
        "\n",
        "print(f\"Q={Q},\\n R={R}\")\n",
        "\n",
        "b = np.array([1,2,7])\n",
        "c = Q.T @ b\n",
        "x = np.linalg.inv(R) @ c\n",
        "print(f\"Solution x={x}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcR9PBXXfmn3"
      },
      "source": [
        "---\n",
        "**Householder tranformations to obtain QR decomposition**\n",
        "\n",
        "(See chapter 5.1 of Golub's Matric Computations for a more extensive discussion)\n",
        "\n",
        "We know that if $u$ is a vector than $H=I-2uu^T$ is a reflection matrix such that $Hv$ reflects $v$ about $u$ and $H$ is orthogonal.\n",
        "\n",
        "So idea is to take $A$ and work column by column by multiplying with suitable $H_1, H_2,...$ such that it gets converted into $R$ which is upper triangular.\n",
        "\n",
        "i.e. $H_{n-1} ... H_1 A = R$\n",
        "\n",
        "Then $Q^T = H_{n-1} ... H_1$\n",
        "\n",
        "Consider what matrix can reflect vector $x$ to $y$.\n",
        "\n",
        "We can find $u = x-y$ and then $u_n = \\frac{u}{||u||}$ is the unit normal then $H_u = I-2u_nu_n^T$ will move $x$ to $y$.\n",
        "\n",
        "This idea is used step by step. Start with first column of $A = \\begin{bmatrix} a_1 & a_2 & ... & a_n \\end{bmatrix}$ which is  $a_1$. We want to move it to $\\begin{bmatrix} ||a_1|| \\\\ 0 \\\\ ... \\\\0 \\end{bmatrix}$ by finding the suitable householder transformation $H_1$, which we can find by the idea in previous para. (Note the reason for the choice of the first element is to preserve the length of $a_1$ as orthogonal matrices preserve that).\n",
        "\n",
        "Thus, $u_1=a_1 - ||a_1|| [1, 0, 0...,0]^T$\n",
        "\n",
        "and $H_1 = I-2u_1 u_1^T /||u_1||^2$\n",
        "\n",
        "and so on.\n",
        "\n",
        "A worked out example below will illustrate further.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efxlt58w7wbb"
      },
      "source": [
        "---\n",
        "**Example: Using Householder method for QR decomposition**\n",
        "\n",
        "\n",
        "Let $A=\\begin{bmatrix} 1 & 2 & 6 \\\\ 4 & 0 & 5 \\\\ 7 & 8 & 9 \\end{bmatrix} = \\begin{bmatrix} c_1 & c_2 & c_3 \\end{bmatrix}$\n",
        "\n",
        "The first column $c_1=\\begin{bmatrix} 1 \\\\ 4 \\\\ 7 \\end{bmatrix}$ has a magnitude $||c_1|| = 8.124$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ku9qhT-8IT1",
        "outputId": "d3d05bdb-e6ab-4966-8e67-ae0d93186740"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1]\n",
            " [4]\n",
            " [7]] 8.12403840463596\n"
          ]
        }
      ],
      "source": [
        "A = np.array([[1,2,6],\n",
        "              [4,0,5],\n",
        "              [7,8,9]])\n",
        "c1 = A[:,0].reshape(3,1)\n",
        "print(c1,np.linalg.norm(c1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxmVslKF8pZq"
      },
      "source": [
        "The first householder vector is $u_1=c_1 - ||c_1||\\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix} = $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPBZXzB-885V",
        "outputId": "86619862-d223-4245-abd8-ff24097129e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-7.1240384]\n",
            " [ 4.       ]\n",
            " [ 7.       ]] 10.758806773556634\n",
            "[[ 50.75192319 -28.49615362 -49.86826883]\n",
            " [-28.49615362  16.          28.        ]\n",
            " [-49.86826883  28.          49.        ]]\n"
          ]
        }
      ],
      "source": [
        "u1 = (c1-np.linalg.norm(c1)*np.array([1,0,0]).reshape(3,1)).reshape(3,1)\n",
        "print(u1, np.linalg.norm(u1))\n",
        "print(u1@u1.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mok-u7Ym9c3X"
      },
      "source": [
        "The first householder matrix is:\n",
        "\n",
        "$H_1 = I-2u_1 u_1^T / ||u_1||^2$\n",
        "\n",
        "$A_1 = H_1 \\times A$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-2xpqbR92lH",
        "outputId": "37fe1e5b-e435-4d7c-b53d-6a5e325d28c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "H1=[[ 0.12309149  0.49236596  0.86164044]\n",
            " [ 0.49236596  0.72354671 -0.48379326]\n",
            " [ 0.86164044 -0.48379326  0.1533618 ]]\n",
            "H1 x A=[[ 8.12403840e+00  7.13930648e+00  1.09551427e+01]\n",
            " [ 8.32667268e-16 -2.88561413e+00  2.21779001e+00]\n",
            " [ 1.66533454e-15  2.95017527e+00  4.13113252e+00]]\n"
          ]
        }
      ],
      "source": [
        "H1 = np.eye(3)-2*u1@u1.T/np.linalg.norm(u1)**2\n",
        "A1 = H1@A\n",
        "print(f\"H1={H1}\")\n",
        "print(f\"H1 x A={A1}\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9Q_bPwK_sqq"
      },
      "source": [
        "---\n",
        "Second householder matrix\n",
        "\n",
        "Now rotate the column below the main diagonal to 0, while preserving length\n",
        "\n",
        "$c_2 = A[1:-1,1]$ //second column from diagonal and below\n",
        "\n",
        "$u_2 = c_2 - ||c_2||\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$\n",
        "\n",
        "$G_2 = I_2 - 2 u_2 u_2^T / ||u_2||^2 $\n",
        "\n",
        "$H_2 = \\begin{bmatrix} 1 & \\bf{0}  \\\\ \\bf{0} & G_2 \\end{bmatrix}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFb141XXBOMk",
        "outputId": "f5f4412a-c7e8-48fe-996a-f2cc64e836e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-2.88561413]\n",
            " [ 2.95017527]]\n",
            "[[-7.01239291]\n",
            " [ 2.95017527]] 7.607705853828992\n",
            "[[-0.6992413   0.71488573]\n",
            " [ 0.71488573  0.6992413 ]]\n",
            "[[ 1.          0.          0.        ]\n",
            " [ 0.         -0.6992413   0.71488573]\n",
            " [ 0.          0.71488573  0.6992413 ]]\n",
            "R=[[ 8.1240384   7.13930648 10.9551427 ]\n",
            " [ 0.          4.12677877  1.40251734]\n",
            " [ 0.          0.          4.4741249 ]] Q = [[ 0.12309149  0.27169184  0.95447998]\n",
            " [ 0.49236596 -0.85179064  0.178965  ]\n",
            " [ 0.86164044  0.44792439 -0.23861999]]\n",
            "Checking orthogonality of Q as:||I-Q Q^T||=9.883854019163205e-16\n",
            "Check the different of Q@R with A as: ||QR-A||= 5.468732061277671e-15\n"
          ]
        }
      ],
      "source": [
        "c2 = A1[1:,1].reshape(2,1)\n",
        "print(c2)\n",
        "u2 = c2-np.linalg.norm(c2)*np.array([1,0]).reshape(2,1)\n",
        "print(u2, np.linalg.norm(u2))\n",
        "G2 = np.eye(2)-2*u2@u2.T/np.linalg.norm(u2)**2\n",
        "print(G2)\n",
        "H2  = np.eye(3)\n",
        "H2[1:,1:] = G2\n",
        "print(H2)\n",
        "A2=H2@A1\n",
        "A2[A2<10e-14]=0\n",
        "R=A2\n",
        "Q=(H2@H1).T\n",
        "print(f\"R={A2} Q = {Q}\")\n",
        "print(f\"Checking orthogonality of Q as:||I-Q Q^T||={np.linalg.norm(Q@Q.T-np.eye(3))}\")\n",
        "print(f\"Check the different of Q@R with A as: ||QR-A||= {np.linalg.norm(Q@R-A)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyMRN25aNTKk"
      },
      "source": [
        "---\n",
        "**Gram-Schmidt and QR decomposition**\n",
        "\n",
        "Let $A = [a_1, a_2, ..., a_n]$ be a mxn matrix\n",
        "\n",
        "Then Gram-Schmidt procedure for orthogonalization is:\n",
        "\n",
        "$b_1 = a_1$ and $q_1 = \\frac{1}{||b_1||} b_1$\n",
        "\n",
        "$b_2 = a_2 - <a_2, q_1>q_1$ and $q_2 = \\frac{1}{||b_2||} b_2$\n",
        "\n",
        "$b_3 = a_3 - <a_3, q_2>q_2 -<a_3, q_1>q_1$ and $q_3 = \\frac{1}{||b_3||} b_3$\n",
        "\n",
        "and so on.\n",
        "\n",
        "We can rearrange and write:\n",
        "\n",
        "$A = [a_1, a_2, ..., a_n] = [q_1, q_2,  ... q_n]\\begin{bmatrix} q_1^Ta_1 & q_1^Ta_2 & ... & q_n^Ta_n \\\\ 0 & q_2^Ta_2 & ... & q_n^Ta_n \\\\ ... \\\\ 0 & 0 & ... & q_n^Ta_n \\end{bmatrix} = QR$\n",
        "\n",
        "$Q$ has orthonormal columns and if $m = n$, Q is an orthogonal matrix.\n",
        "\n",
        "Solving least squares becomes easy as ⁉\n",
        "\n",
        "$Ax^{'} = QRx^{'} = b$ and $Rx^{'} = Q^Tb$ can be solved by backward substitution\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "102cWaJ5KQyZ"
      },
      "source": [
        "---\n",
        "**Example: We will use Gram-Schmidt for QR decomposition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLVBAoY4KaXX",
        "outputId": "ffef1e59-66a7-4a6a-f9be-a68b882644e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.6653345369377348e-16\n",
            "[[ 0.12309149  0.27169184  0.95447998]\n",
            " [ 0.49236596 -0.85179064  0.178965  ]\n",
            " [ 0.86164044  0.44792439 -0.23861999]]\n",
            "[[ 8.1240384   7.13930648 10.9551427 ]\n",
            " [ 0.          4.12677877  1.40251734]\n",
            " [ 0.          0.          4.4741249 ]]\n",
            "||QxQ^T-I||=7.986715129536681e-16\n",
            "||QR-A||=9.057664235374552e-16\n"
          ]
        }
      ],
      "source": [
        "A = np.array([[1,2,6],\n",
        "              [4,0,5],\n",
        "              [7,8,9]])\n",
        "\n",
        "Q=np.zeros((3,3))\n",
        "R=np.zeros((3,3))\n",
        "\n",
        "#step 1\n",
        "b = A[:,0]\n",
        "Q[:,0] = b/np.linalg.norm(b)\n",
        "R[0,0] = np.linalg.norm(b)\n",
        "\n",
        "#step 2\n",
        "b = (A[:,1]-(A[:,1].T@Q[:,0])*Q[:,0])\n",
        "Q[:,1] = b/np.linalg.norm(b)\n",
        "\n",
        "print(Q[:,0].T @ Q[:,1])\n",
        "\n",
        "R[0,1] = Q[:,0].T@A[:,1]\n",
        "R[1,1] = np.linalg.norm(b)\n",
        "\n",
        "#step 3\n",
        "b = (A[:,2]-(A[:,2].T@Q[:,0])*Q[:,0]-(A[:,2]@Q[:,1])*Q[:,1])\n",
        "Q[:,2] = b/np.linalg.norm(b)\n",
        "\n",
        "R[0,2] = Q[:,0].T@A[:,2]\n",
        "R[1,2] = Q[:,1].T@A[:,2]\n",
        "R[2,2] = np.linalg.norm(b)\n",
        "\n",
        "print(Q)\n",
        "print(R)\n",
        "\n",
        "print(f\"||QxQ^T-I||={np.linalg.norm(Q@Q.T-np.eye(3))}\")\n",
        "\n",
        "print(f\"||QR-A||={np.linalg.norm(Q@R-A)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
